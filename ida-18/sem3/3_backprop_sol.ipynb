{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Современные методы машинного обучения, ИАД\n",
    "\n",
    "## Семинар 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### О задании\n",
    "\n",
    "Задание посвящено реализации различных слоёв нейронной сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. Реализация слоёв графа вычислений\n",
    "\n",
    "В этом задании мы реализуем граф вычислений для задачи распознавания изображений рукописных цифр на примере датасета [MNIST](http://yann.lecun.com/exdb/mnist/) — в частности, эта часть посвящена реализации всех требующихся для построения графа слоёв.\n",
    "\n",
    "Указанная задача является задачей классификации на $K = 10$ классов, поэтому будем строить граф вычислений, выходной слой которого будет содержать 10 нейронов, $k$-ый из которых вычисляет оценку принадлежности объекта $k$-ому классу. В качестве функционала качества в данной задаче будем использовать **кросс-энтропию**:\n",
    "\n",
    "$$Q(a, X) = -\\frac{1}{l}\\sum_{i=1}^l \\sum_{k=1}^K [y_i = k] \\log a_k(x_i),$$\n",
    "где\n",
    "\n",
    "$X = \\{ (x_i, y_i)\\}_{i=1}^l, \\, y_i \\in \\{1, \\dots, K\\},$ — обучающая выборка,\n",
    "\n",
    "$a(x) = (a_k(x))_{k=1}^K \\in \\mathbb{R}^K$ — прогноз графа вычислений для объекта $x$, состоящий из выходов $K$ нейронов выходного слоя (т.е. $a_k(x)$ — оценка принадлежности объекта $x$ классу $k$, построенная при помощи заданного графа вычислений).\n",
    "\n",
    "Нейрнонные сети обучаются с использованием стохастических методов оптимизации, однако для ускорения обучения и большей стабильности за один проход параметры оптимизируются по батчу — набору из нескольких тренировочных примеров, так же batch_size является дополнительной размерностью для входящих в слой тензоров.\n",
    "\n",
    "Для начала определим класс Layer, реализующий тождественный слой, который будет являться базовым классом для всех последующих."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    A building block. Each layer is capable of performing two things:\n",
    "\n",
    "    - Process input to get output:           output = layer.forward(input)\n",
    "\n",
    "    - Propagate gradients through itself:    grad_input = layer.backward(input, grad_output)\n",
    "\n",
    "    Some layers also have learnable parameters which they update during layer.backward.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Here you can initialize layer parameters (if any) and auxiliary stuff.\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError(\"Not implemented in interface\")\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Takes input data of shape [batch, ...], returns output data [batch, ...]\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError(\"Not implemented in interface\")\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"\n",
    "        Performs a backpropagation step through the layer, with respect to the given input.\n",
    "\n",
    "        To compute loss gradients w.r.t input, you need to apply chain rule (backprop):\n",
    "\n",
    "        d loss / d x  = (d loss / d layer) * (d layer / d x)\n",
    "\n",
    "        Luckily, you already receive d loss / d layer as input, so you only need to multiply it by d layer / d x.\n",
    "\n",
    "        If your layer has parameters (e.g. dense layer), you also need to update them here using d loss / d layer\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError(\"Not implemented in interface\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 1**\n",
    "\n",
    "Используя приведенные прототипы, реализуйте слой, применяющий функцию активации ReLU (Rectified Linear Unit) поэлементно к каждому из входов слоя:\n",
    "$$\\text{ReLU}(z) = \\max (0, z)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        ReLU layer simply applies elementwise rectified linear unit to all inputs\n",
    "        This layer does not have any parameters.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Perform ReLU transformation\n",
    "        input shape: [batch, input_units]\n",
    "        output shape: [batch, input_units]\n",
    "        \"\"\"\n",
    "        output = np.maximum(0, input)\n",
    "        return output\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"\n",
    "        Compute gradient of loss w.r.t. ReLU input\n",
    "        \"\"\"\n",
    "        relu_grad_mask = input > 0\n",
    "        return grad_output * relu_grad_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 2**\n",
    "\n",
    "Используя указанные прототипы, реализуйте полносвязный слой, выход которого вычисляется следующим образом (подробнее в соответствующей [лекции](https://github.com/esokolov/ml-course-hse/blob/master/2017-fall/lecture-notes/lecture11-dl.pdf)):\n",
    "\n",
    "$$f(v; W, b)= Wv + b, $$\n",
    "\n",
    "где\n",
    "* v — выход предыдущего слоя (вектор размера num_inputs);\n",
    "* W — матрица весов [num_inputs, num_outputs];\n",
    "* b — столбец свободных членов (вектор размера num_outputs).\n",
    "\n",
    "При создании полносвязного слоя веса $W, \\; b$ необходимо проинициализировать веса с помощью GLOROT (какой именно вариант неважно). Про GLOROT можно прочитать здесь:\n",
    "1. Простой пост: http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization\n",
    "2. Статья с математикой: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n",
    "\n",
    "При каждом вызове backward() необходимо расчитать градиенты по выходу, используя chain-rule, и сделать один шаг градиентного спуска."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "\n",
    "    def __init__(self, input_units, output_units, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        A dense layer is a layer which performs a learned affine transformation:\n",
    "        f(x) = Wx + b\n",
    "\n",
    "        W: matrix of shape [num_inputs, num_outputs]\n",
    "        b: vector of shape [num_outputs]\n",
    "        \"\"\"\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # initialize weights with small random numbers from normal distribution\n",
    "\n",
    "        self.weights = np.random.randn(input_units, output_units) * 0.01\n",
    "        self.biases = np.zeros(output_units)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Perform an affine transformation:\n",
    "        f(x) = <W*x> + b\n",
    "\n",
    "        input shape: [batch, input_units]\n",
    "        output shape: [batch, output units]\n",
    "        \"\"\"\n",
    "        output_units = np.dot(input, self.weights) + self.biases\n",
    "        return output_units\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"\n",
    "        Computes d f / d x = d f / d dense * d dense / d x,\n",
    "        where d dense/ d x = weights transposed, and performs\n",
    "        one step of gradient descent on W and b.\n",
    "\n",
    "        input shape: [batch, input_units]\n",
    "        grad_output: [batch, output units]\n",
    "\n",
    "        Returns: grad_input, gradient of output w.r.t input\n",
    "        \"\"\"\n",
    "        grad_input = grad_output.dot(self.weights.T)\n",
    "        grad_weights = input.T.dot(grad_output)\n",
    "        grad_biases = grad_output.sum(0)\n",
    "\n",
    "        assert grad_weights.shape == self.weights.shape and \\\n",
    "            grad_biases.shape == self.biases.shape\n",
    "\n",
    "        self.weights = self.weights - self.learning_rate * grad_weights\n",
    "        self.biases = self.biases - self.learning_rate * grad_biases\n",
    "\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3**\n",
    "\n",
    "Как было сказано ранее, в качестве функционала качества в данной задаче мы будем использовать кросс-энтропию. Используя прототипы ниже, реализуйте вычисление данного функционала и его градиента по выходам графа вычислений.\n",
    "\n",
    "Кросс-энтропия предполагает, что модель для каждого объекта выдает вероятности принадлежности к каждому из $K$ классов, т.е. что для одного объекта все $K$ вероятностей неотрицательны и суммируются в 1. В нашем же случае в построении графа участвуют только полносвязный и ReLU слои, а потому выходы графа не являются вероятностями — как правило, в этом случае прогноз $a(x)$ модели нормируется при помощи функции softmax следующим образом:\n",
    "\n",
    "$$\\text{softmax}(a_k(x)) = \\frac{\\exp(a_k(x))}{\\sum_{k=1}^K \\exp(a_k(x))}.$$\n",
    "\n",
    "При реализации указанных функций предполагается, что переданные в качестве параметров оценки принадлежности объектов классам не являются нормированными (их еще называют логитами), но при вычислении указанных величин используйте указанное выше преобразование для приведения этих оценок к корректному виду."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_crossentropy_with_logits(logits, y_true):\n",
    "    \"\"\"\n",
    "    Compute crossentropy from logits and ids of correct answers\n",
    "    logits shape: [batch_size, num_classes]\n",
    "    y_true: [batch_size]\n",
    "    output is a number\n",
    "    \"\"\"\n",
    "    logits_for_answers = logits[np.arange(len(logits)), y_true]\n",
    "    xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits), axis=-1))\n",
    "    return xentropy\n",
    "\n",
    "\n",
    "def grad_softmax_crossentropy_with_logits(logits, y_true):\n",
    "    \"\"\"\n",
    "    Compute crossentropy gradient from logits and ids of correct answers\n",
    "    logits shape: [batch_size, num_classes]\n",
    "    y_true: [batch_size]\n",
    "    \"\"\"\n",
    "    ones_for_answers = np.zeros_like(logits)\n",
    "    ones_for_answers[np.arange(len(logits)), y_true] = 1\n",
    "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1, keepdims=True)\n",
    "    return (- ones_for_answers + softmax) / logits.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. Реализация и применение графа вычислений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой части мы научимся объединять слои в единый граф вычислений, а также использовать его для прямого прохода (вычисления прогнозов на объектах) и обратного прохода (обновление обучаемых параметров графа), после чего у нас появится возможность обучить граф. Для простоты реализации будем считать, что в нашем случае граф вычислений задается как список (python list) слоёв из числа реализованных ранее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже приведен код для скачивания датасета MNIST с официального сайта. Датасет делится на тренировочную и тестовую части. Тренировочная дополнительно разбивается на тренировочную и валидационную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import gzip\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "def load_mnist(flatten=False):\n",
    "    \"\"\"taken from https://github.com/Lasagne/Lasagne/blob/master/examples/mnist.py\"\"\"\n",
    "    # We first define a download function, supporting both Python 2 and 3.\n",
    "\n",
    "    def download(filename, source='http://yann.lecun.com/exdb/mnist/'):\n",
    "        print(\"Downloading %s\" % filename)\n",
    "        urlretrieve(source + filename, filename)\n",
    "\n",
    "    # We then define functions for loading MNIST images and labels.\n",
    "    # For convenience, they also download the requested files if needed.\n",
    "\n",
    "    def load_mnist_images(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        # Read the inputs in Yann LeCun's binary format.\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "        # The inputs are vectors now, we reshape them to monochrome 2D images,\n",
    "        # following the shape convention: (examples, channels, rows, columns)\n",
    "        data = data.reshape(-1, 1, 28, 28)\n",
    "        # The inputs come as bytes, we convert them to float32 in range [0,1].\n",
    "        # (Actually to range [0, 255/256], for compatibility to the version\n",
    "        # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.)\n",
    "        return data / np.float32(256)\n",
    "\n",
    "    def load_mnist_labels(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        # Read the labels in Yann LeCun's binary format.\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "        # The labels are vectors of integers now, that's exactly what we want.\n",
    "        return data\n",
    "\n",
    "    # We can now download and read the training and test set images and labels.\n",
    "    X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
    "    y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
    "    X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
    "    y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "    # We reserve the last 10000 training examples for validation.\n",
    "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
    "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
    "    \n",
    "    if flatten:\n",
    "        X_train = X_train.reshape([X_train.shape[0], -1])\n",
    "        X_val = X_val.reshape([X_val.shape[0], -1])\n",
    "        X_test = X_test.reshape([X_test.shape[0], -1])\n",
    "\n",
    "    # We just return all the arrays in order, as expected in main().\n",
    "    # (It doesn't matter how we do this as long as we can read them again.)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на несколько объектов из этого датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x234f1769cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_mnist(flatten=True)\n",
    "\n",
    "plt.figure(figsize=[6, 6])\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.title(\"Label: %i\"%y_train[i])\n",
    "    plt.imshow(X_train[i].reshape([28, 28]),cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 4 (2 балла).**\n",
    "\n",
    "Используя прототип ниже, реализуйте прямой и обратный проход по графу вычислений и функцию для получения предсказаний метки класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        \"\"\"\n",
    "        layers — list of Layer objects\n",
    "        \"\"\"\n",
    "        \n",
    "        self.layers = layers\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Compute activations of all network layers by applying them sequentially.\n",
    "        Return a list of activations for each layer. \n",
    "        Make sure last activation corresponds to network logits.\n",
    "        \"\"\"\n",
    "        \n",
    "        activations = []\n",
    "        input = X\n",
    "\n",
    "        for i in range(len(self.layers)):\n",
    "            activations.append(self.layers[i].forward(input))\n",
    "            input = activations[-1]\n",
    "\n",
    "        assert len(activations) == len(self.layers)\n",
    "        return activations\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use network to predict the most likely class for each sample.\n",
    "        \"\"\"\n",
    "        return np.argmax(self.forward(X)[-1], axis=1)\n",
    "        \n",
    "    def backward(self, X, y):\n",
    "        \"\"\"\n",
    "        Train your network on a given batch of X and y.\n",
    "        You first need to run forward to get all layer activations.\n",
    "        Then you can run layer.backward going from last to first layer.\n",
    "\n",
    "        After you called backward for all layers, all Dense layers have already made one gradient step.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the layer activations\n",
    "        layer_activations = self.forward(X)\n",
    "        layer_inputs = [X] + layer_activations  # layer_input[i] is an input for network[i]\n",
    "        logits = layer_activations[-1]\n",
    "\n",
    "        # Compute the loss and the initial gradient\n",
    "        loss = softmax_crossentropy_with_logits(logits, y)\n",
    "        loss_grad = grad_softmax_crossentropy_with_logits(logits, y)\n",
    "\n",
    "        # propagate gradients through network layers using .backward\n",
    "        # hint: start from last layer and move to earlier layers\n",
    "        input_grad = loss_grad\n",
    "        for i in range(len(layer_activations) - 1, -1, -1):\n",
    "            input_grad = self.layers[i].backward(layer_inputs[i], input_grad)\n",
    "\n",
    "        return np.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = []\n",
    "hidden_layers_size = 40\n",
    "layers.append(Dense(X_train.shape[1], hidden_layers_size))\n",
    "layers.append(ReLU())\n",
    "layers.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "layers.append(ReLU())\n",
    "layers.append(Dense(hidden_layers_size, 10))\n",
    "\n",
    "model = NeuralNetwork(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все готово для запуска обучения. Если все реализовано корректно, то точность классификации на валидационном множестве должна превысить 97%. \n",
    "\n",
    "Ниже определена функции для итерации по батчам, принимающая на вход картинки, матки классов, а также размер батча и флаг отвечающий за перемешивание примеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False, seed=1234):\n",
    "    assert len(inputs) == len(targets)\n",
    "    \n",
    "    indices = np.arange(len(inputs)).astype(np.int32)\n",
    "    if shuffle:\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    for start_idx in trange(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        batch = indices[start_idx:start_idx + batchsize]\n",
    "        \n",
    "        yield inputs[batch], targets[batch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже приведены функции для обучения модели и отслеживания значения loss на тренироворочной части и на валидации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "train_log = []\n",
    "val_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n",
      "Train accuracy: 0.97056\n",
      "Val accuracy: 0.9642\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2348b4774e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|███████████████████████████████████████████████████████████████              | 1279/1562 [00:01<00:00, 858.95it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-c53b1c2f21d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterate_minibatches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtrain_log\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-d8165ddccbcd>\u001b[0m in \u001b[0;36miterate_minibatches\u001b[1;34m(inputs, targets, batchsize, shuffle, seed)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstart_idx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[1;32myield\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(15):\n",
    "    for x_batch, y_batch in iterate_minibatches(X_train, y_train, batchsize=32, shuffle=True):\n",
    "        model.backward(x_batch, y_batch)\n",
    "    \n",
    "    train_log.append(np.mean(model.predict(X_train) == y_train))\n",
    "    val_log.append(np.mean(model.predict(X_val) == y_val))\n",
    "    \n",
    "    clear_output()\n",
    "    print(\"Epoch\", epoch)\n",
    "    print(\"Train accuracy:\", train_log[-1])\n",
    "    print(\"Val accuracy:\", val_log[-1])\n",
    "    plt.plot(train_log, label='train accuracy')\n",
    "    plt.plot(val_log, label='val accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3. Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой части мы увидим как с помощью фреймворков с автоматическим дифференцированием, создание нейронных сетей занимает намного меньше времени."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\panov\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/15\n",
      "50000/50000 [==============================] - ETA: 9:31 - loss: 2.3091 - acc: 0.031 - ETA: 24s - loss: 2.3240 - acc: 0.129 - ETA: 16s - loss: 2.2640 - acc: 0.17 - ETA: 11s - loss: 2.2030 - acc: 0.21 - ETA: 8s - loss: 2.1415 - acc: 0.2786 - ETA: 7s - loss: 2.0881 - acc: 0.323 - ETA: 6s - loss: 2.0259 - acc: 0.366 - ETA: 6s - loss: 1.9843 - acc: 0.392 - ETA: 5s - loss: 1.9441 - acc: 0.415 - ETA: 5s - loss: 1.9001 - acc: 0.438 - ETA: 5s - loss: 1.8673 - acc: 0.453 - ETA: 5s - loss: 1.8192 - acc: 0.475 - ETA: 5s - loss: 1.7879 - acc: 0.489 - ETA: 4s - loss: 1.7381 - acc: 0.508 - ETA: 4s - loss: 1.6936 - acc: 0.525 - ETA: 4s - loss: 1.6408 - acc: 0.542 - ETA: 4s - loss: 1.5949 - acc: 0.556 - ETA: 4s - loss: 1.5395 - acc: 0.574 - ETA: 3s - loss: 1.5020 - acc: 0.585 - ETA: 3s - loss: 1.4747 - acc: 0.594 - ETA: 3s - loss: 1.4319 - acc: 0.608 - ETA: 3s - loss: 1.3949 - acc: 0.618 - ETA: 3s - loss: 1.3647 - acc: 0.626 - ETA: 3s - loss: 1.3368 - acc: 0.633 - ETA: 3s - loss: 1.3193 - acc: 0.639 - ETA: 3s - loss: 1.3070 - acc: 0.643 - ETA: 3s - loss: 1.2917 - acc: 0.647 - ETA: 3s - loss: 1.2641 - acc: 0.654 - ETA: 2s - loss: 1.2328 - acc: 0.663 - ETA: 2s - loss: 1.2119 - acc: 0.669 - ETA: 2s - loss: 1.1853 - acc: 0.676 - ETA: 2s - loss: 1.1611 - acc: 0.684 - ETA: 2s - loss: 1.1459 - acc: 0.688 - ETA: 2s - loss: 1.1220 - acc: 0.694 - ETA: 2s - loss: 1.1003 - acc: 0.701 - ETA: 2s - loss: 1.0784 - acc: 0.707 - ETA: 2s - loss: 1.0605 - acc: 0.712 - ETA: 2s - loss: 1.0425 - acc: 0.716 - ETA: 1s - loss: 1.0263 - acc: 0.721 - ETA: 1s - loss: 1.0129 - acc: 0.724 - ETA: 1s - loss: 0.9989 - acc: 0.728 - ETA: 1s - loss: 0.9870 - acc: 0.731 - ETA: 1s - loss: 0.9740 - acc: 0.735 - ETA: 1s - loss: 0.9601 - acc: 0.738 - ETA: 1s - loss: 0.9463 - acc: 0.742 - ETA: 1s - loss: 0.9322 - acc: 0.745 - ETA: 1s - loss: 0.9200 - acc: 0.749 - ETA: 1s - loss: 0.9135 - acc: 0.750 - ETA: 1s - loss: 0.9016 - acc: 0.754 - ETA: 1s - loss: 0.8883 - acc: 0.757 - ETA: 1s - loss: 0.8769 - acc: 0.761 - ETA: 0s - loss: 0.8657 - acc: 0.763 - ETA: 0s - loss: 0.8557 - acc: 0.766 - ETA: 0s - loss: 0.8454 - acc: 0.768 - ETA: 0s - loss: 0.8368 - acc: 0.771 - ETA: 0s - loss: 0.8303 - acc: 0.772 - ETA: 0s - loss: 0.8227 - acc: 0.774 - ETA: 0s - loss: 0.8134 - acc: 0.776 - ETA: 0s - loss: 0.8058 - acc: 0.778 - ETA: 0s - loss: 0.7984 - acc: 0.780 - ETA: 0s - loss: 0.7916 - acc: 0.782 - ETA: 0s - loss: 0.7839 - acc: 0.784 - ETA: 0s - loss: 0.7803 - acc: 0.785 - ETA: 0s - loss: 0.7746 - acc: 0.786 - ETA: 0s - loss: 0.7678 - acc: 0.788 - ETA: 0s - loss: 0.7593 - acc: 0.790 - 4s 79us/step - loss: 0.7574 - acc: 0.7914 - val_loss: 0.3691 - val_acc: 0.8954\n",
      "Epoch 2/15\n",
      "50000/50000 [==============================] - ETA: 7s - loss: 0.2566 - acc: 0.937 - ETA: 2s - loss: 0.3513 - acc: 0.904 - ETA: 2s - loss: 0.3756 - acc: 0.903 - ETA: 3s - loss: 0.3749 - acc: 0.901 - ETA: 3s - loss: 0.3900 - acc: 0.891 - ETA: 2s - loss: 0.3918 - acc: 0.890 - ETA: 2s - loss: 0.3945 - acc: 0.887 - ETA: 2s - loss: 0.3917 - acc: 0.889 - ETA: 2s - loss: 0.3891 - acc: 0.890 - ETA: 2s - loss: 0.3862 - acc: 0.890 - ETA: 2s - loss: 0.3818 - acc: 0.892 - ETA: 2s - loss: 0.3819 - acc: 0.892 - ETA: 2s - loss: 0.3833 - acc: 0.892 - ETA: 2s - loss: 0.3854 - acc: 0.892 - ETA: 2s - loss: 0.3854 - acc: 0.892 - ETA: 2s - loss: 0.3834 - acc: 0.892 - ETA: 2s - loss: 0.3815 - acc: 0.892 - ETA: 2s - loss: 0.3821 - acc: 0.892 - ETA: 2s - loss: 0.3786 - acc: 0.893 - ETA: 2s - loss: 0.3764 - acc: 0.894 - ETA: 2s - loss: 0.3770 - acc: 0.894 - ETA: 2s - loss: 0.3744 - acc: 0.894 - ETA: 2s - loss: 0.3725 - acc: 0.895 - ETA: 2s - loss: 0.3712 - acc: 0.895 - ETA: 2s - loss: 0.3714 - acc: 0.894 - ETA: 2s - loss: 0.3732 - acc: 0.893 - ETA: 2s - loss: 0.3709 - acc: 0.894 - ETA: 1s - loss: 0.3704 - acc: 0.894 - ETA: 1s - loss: 0.3669 - acc: 0.895 - ETA: 1s - loss: 0.3663 - acc: 0.895 - ETA: 1s - loss: 0.3684 - acc: 0.895 - ETA: 1s - loss: 0.3666 - acc: 0.896 - ETA: 1s - loss: 0.3665 - acc: 0.896 - ETA: 1s - loss: 0.3632 - acc: 0.897 - ETA: 1s - loss: 0.3613 - acc: 0.898 - ETA: 1s - loss: 0.3609 - acc: 0.898 - ETA: 1s - loss: 0.3616 - acc: 0.898 - ETA: 1s - loss: 0.3599 - acc: 0.898 - ETA: 1s - loss: 0.3592 - acc: 0.899 - ETA: 1s - loss: 0.3596 - acc: 0.898 - ETA: 1s - loss: 0.3580 - acc: 0.898 - ETA: 1s - loss: 0.3566 - acc: 0.899 - ETA: 1s - loss: 0.3568 - acc: 0.899 - ETA: 1s - loss: 0.3560 - acc: 0.899 - ETA: 0s - loss: 0.3544 - acc: 0.899 - ETA: 0s - loss: 0.3540 - acc: 0.899 - ETA: 0s - loss: 0.3533 - acc: 0.899 - ETA: 0s - loss: 0.3514 - acc: 0.900 - ETA: 0s - loss: 0.3508 - acc: 0.900 - ETA: 0s - loss: 0.3500 - acc: 0.900 - ETA: 0s - loss: 0.3493 - acc: 0.900 - ETA: 0s - loss: 0.3486 - acc: 0.900 - ETA: 0s - loss: 0.3480 - acc: 0.900 - ETA: 0s - loss: 0.3470 - acc: 0.901 - ETA: 0s - loss: 0.3459 - acc: 0.901 - ETA: 0s - loss: 0.3448 - acc: 0.901 - ETA: 0s - loss: 0.3442 - acc: 0.901 - ETA: 0s - loss: 0.3435 - acc: 0.902 - ETA: 0s - loss: 0.3431 - acc: 0.902 - ETA: 0s - loss: 0.3428 - acc: 0.902 - ETA: 0s - loss: 0.3421 - acc: 0.902 - 3s 66us/step - loss: 0.3411 - acc: 0.9026 - val_loss: 0.2908 - val_acc: 0.9156\n",
      "Epoch 3/15\n",
      "50000/50000 [==============================] - ETA: 7s - loss: 0.4990 - acc: 0.812 - ETA: 2s - loss: 0.2950 - acc: 0.916 - ETA: 2s - loss: 0.3199 - acc: 0.912 - ETA: 2s - loss: 0.3051 - acc: 0.916 - ETA: 2s - loss: 0.3012 - acc: 0.915 - ETA: 2s - loss: 0.2950 - acc: 0.916 - ETA: 2s - loss: 0.2936 - acc: 0.917 - ETA: 2s - loss: 0.2936 - acc: 0.916 - ETA: 2s - loss: 0.2951 - acc: 0.916 - ETA: 2s - loss: 0.2966 - acc: 0.915 - ETA: 2s - loss: 0.2951 - acc: 0.916 - ETA: 2s - loss: 0.2936 - acc: 0.916 - ETA: 2s - loss: 0.2901 - acc: 0.917 - ETA: 2s - loss: 0.2909 - acc: 0.917 - ETA: 2s - loss: 0.2966 - acc: 0.915 - ETA: 2s - loss: 0.2973 - acc: 0.915 - ETA: 2s - loss: 0.2936 - acc: 0.915 - ETA: 2s - loss: 0.2953 - acc: 0.914 - ETA: 1s - loss: 0.2955 - acc: 0.914 - ETA: 1s - loss: 0.2952 - acc: 0.914 - ETA: 1s - loss: 0.2943 - acc: 0.914 - ETA: 1s - loss: 0.2953 - acc: 0.914 - ETA: 1s - loss: 0.2946 - acc: 0.915 - ETA: 1s - loss: 0.2944 - acc: 0.915 - ETA: 1s - loss: 0.2949 - acc: 0.914 - ETA: 1s - loss: 0.2926 - acc: 0.915 - ETA: 1s - loss: 0.2924 - acc: 0.915 - ETA: 1s - loss: 0.2914 - acc: 0.916 - ETA: 1s - loss: 0.2925 - acc: 0.915 - ETA: 1s - loss: 0.2935 - acc: 0.915 - ETA: 1s - loss: 0.2924 - acc: 0.915 - ETA: 1s - loss: 0.2925 - acc: 0.915 - ETA: 1s - loss: 0.2926 - acc: 0.915 - ETA: 1s - loss: 0.2915 - acc: 0.915 - ETA: 1s - loss: 0.2903 - acc: 0.916 - ETA: 1s - loss: 0.2896 - acc: 0.916 - ETA: 1s - loss: 0.2898 - acc: 0.916 - ETA: 1s - loss: 0.2896 - acc: 0.916 - ETA: 1s - loss: 0.2897 - acc: 0.916 - ETA: 1s - loss: 0.2897 - acc: 0.916 - ETA: 0s - loss: 0.2890 - acc: 0.916 - ETA: 0s - loss: 0.2892 - acc: 0.916 - ETA: 0s - loss: 0.2885 - acc: 0.916 - ETA: 0s - loss: 0.2880 - acc: 0.916 - ETA: 0s - loss: 0.2879 - acc: 0.916 - ETA: 0s - loss: 0.2872 - acc: 0.916 - ETA: 0s - loss: 0.2863 - acc: 0.917 - ETA: 0s - loss: 0.2863 - acc: 0.917 - ETA: 0s - loss: 0.2854 - acc: 0.917 - ETA: 0s - loss: 0.2842 - acc: 0.917 - ETA: 0s - loss: 0.2840 - acc: 0.917 - ETA: 0s - loss: 0.2841 - acc: 0.918 - ETA: 0s - loss: 0.2826 - acc: 0.918 - ETA: 0s - loss: 0.2825 - acc: 0.918 - ETA: 0s - loss: 0.2832 - acc: 0.918 - ETA: 0s - loss: 0.2824 - acc: 0.918 - ETA: 0s - loss: 0.2825 - acc: 0.918 - ETA: 0s - loss: 0.2824 - acc: 0.918 - ETA: 0s - loss: 0.2817 - acc: 0.918 - ETA: 0s - loss: 0.2829 - acc: 0.918 - 3s 66us/step - loss: 0.2832 - acc: 0.9183 - val_loss: 0.2517 - val_acc: 0.9303\n",
      "Epoch 4/15\n",
      "50000/50000 [==============================] - ETA: 9s - loss: 0.3347 - acc: 0.937 - ETA: 3s - loss: 0.2821 - acc: 0.912 - ETA: 2s - loss: 0.2794 - acc: 0.916 - ETA: 2s - loss: 0.2677 - acc: 0.919 - ETA: 2s - loss: 0.2494 - acc: 0.925 - ETA: 2s - loss: 0.2453 - acc: 0.927 - ETA: 2s - loss: 0.2426 - acc: 0.928 - ETA: 2s - loss: 0.2458 - acc: 0.927 - ETA: 3s - loss: 0.2491 - acc: 0.926 - ETA: 3s - loss: 0.2514 - acc: 0.925 - ETA: 3s - loss: 0.2489 - acc: 0.925 - ETA: 2s - loss: 0.2484 - acc: 0.926 - ETA: 2s - loss: 0.2487 - acc: 0.926 - ETA: 2s - loss: 0.2504 - acc: 0.925 - ETA: 2s - loss: 0.2465 - acc: 0.927 - ETA: 2s - loss: 0.2477 - acc: 0.926 - ETA: 2s - loss: 0.2479 - acc: 0.926 - ETA: 2s - loss: 0.2487 - acc: 0.926 - ETA: 2s - loss: 0.2484 - acc: 0.926 - ETA: 2s - loss: 0.2493 - acc: 0.926 - ETA: 2s - loss: 0.2499 - acc: 0.926 - ETA: 2s - loss: 0.2514 - acc: 0.925 - ETA: 2s - loss: 0.2524 - acc: 0.926 - ETA: 2s - loss: 0.2503 - acc: 0.926 - ETA: 2s - loss: 0.2494 - acc: 0.926 - ETA: 2s - loss: 0.2511 - acc: 0.926 - ETA: 2s - loss: 0.2517 - acc: 0.926 - ETA: 2s - loss: 0.2508 - acc: 0.926 - ETA: 2s - loss: 0.2512 - acc: 0.926 - ETA: 2s - loss: 0.2514 - acc: 0.927 - ETA: 2s - loss: 0.2533 - acc: 0.926 - ETA: 1s - loss: 0.2537 - acc: 0.926 - ETA: 1s - loss: 0.2529 - acc: 0.926 - ETA: 1s - loss: 0.2528 - acc: 0.926 - ETA: 1s - loss: 0.2526 - acc: 0.927 - ETA: 1s - loss: 0.2526 - acc: 0.927 - ETA: 1s - loss: 0.2523 - acc: 0.927 - ETA: 1s - loss: 0.2527 - acc: 0.926 - ETA: 1s - loss: 0.2516 - acc: 0.927 - ETA: 1s - loss: 0.2520 - acc: 0.927 - ETA: 1s - loss: 0.2519 - acc: 0.927 - ETA: 1s - loss: 0.2516 - acc: 0.927 - ETA: 1s - loss: 0.2514 - acc: 0.927 - ETA: 1s - loss: 0.2522 - acc: 0.926 - ETA: 1s - loss: 0.2524 - acc: 0.926 - ETA: 1s - loss: 0.2529 - acc: 0.926 - ETA: 1s - loss: 0.2517 - acc: 0.927 - ETA: 1s - loss: 0.2517 - acc: 0.926 - ETA: 1s - loss: 0.2516 - acc: 0.927 - ETA: 1s - loss: 0.2512 - acc: 0.926 - ETA: 0s - loss: 0.2512 - acc: 0.927 - ETA: 0s - loss: 0.2505 - acc: 0.927 - ETA: 0s - loss: 0.2503 - acc: 0.927 - ETA: 0s - loss: 0.2503 - acc: 0.927 - ETA: 0s - loss: 0.2501 - acc: 0.927 - ETA: 0s - loss: 0.2501 - acc: 0.927 - ETA: 0s - loss: 0.2504 - acc: 0.927 - ETA: 0s - loss: 0.2504 - acc: 0.927 - ETA: 0s - loss: 0.2503 - acc: 0.927 - ETA: 0s - loss: 0.2500 - acc: 0.927 - ETA: 0s - loss: 0.2499 - acc: 0.927 - ETA: 0s - loss: 0.2504 - acc: 0.927 - ETA: 0s - loss: 0.2503 - acc: 0.927 - ETA: 0s - loss: 0.2507 - acc: 0.927 - ETA: 0s - loss: 0.2504 - acc: 0.927 - ETA: 0s - loss: 0.2505 - acc: 0.927 - ETA: 0s - loss: 0.2507 - acc: 0.927 - ETA: 0s - loss: 0.2503 - acc: 0.927 - ETA: 0s - loss: 0.2512 - acc: 0.927 - ETA: 0s - loss: 0.2513 - acc: 0.927 - ETA: 0s - loss: 0.2513 - acc: 0.927 - 4s 76us/step - loss: 0.2511 - acc: 0.9273 - val_loss: 0.2284 - val_acc: 0.9329\n",
      "Epoch 5/15\n",
      "50000/50000 [==============================] - ETA: 7s - loss: 0.1517 - acc: 0.937 - ETA: 4s - loss: 0.2867 - acc: 0.911 - ETA: 5s - loss: 0.2508 - acc: 0.925 - ETA: 5s - loss: 0.2344 - acc: 0.933 - ETA: 5s - loss: 0.2435 - acc: 0.930 - ETA: 5s - loss: 0.2354 - acc: 0.930 - ETA: 4s - loss: 0.2253 - acc: 0.932 - ETA: 4s - loss: 0.2213 - acc: 0.933 - ETA: 3s - loss: 0.2249 - acc: 0.930 - ETA: 3s - loss: 0.2251 - acc: 0.930 - ETA: 3s - loss: 0.2249 - acc: 0.931 - ETA: 3s - loss: 0.2295 - acc: 0.930 - ETA: 3s - loss: 0.2254 - acc: 0.931 - ETA: 3s - loss: 0.2256 - acc: 0.931 - ETA: 3s - loss: 0.2239 - acc: 0.932 - ETA: 2s - loss: 0.2291 - acc: 0.931 - ETA: 2s - loss: 0.2304 - acc: 0.932 - ETA: 2s - loss: 0.2351 - acc: 0.931 - ETA: 2s - loss: 0.2356 - acc: 0.932 - ETA: 2s - loss: 0.2369 - acc: 0.931 - ETA: 2s - loss: 0.2355 - acc: 0.931 - ETA: 2s - loss: 0.2347 - acc: 0.931 - ETA: 2s - loss: 0.2350 - acc: 0.931 - ETA: 2s - loss: 0.2335 - acc: 0.931 - ETA: 2s - loss: 0.2335 - acc: 0.932 - ETA: 2s - loss: 0.2329 - acc: 0.932 - ETA: 1s - loss: 0.2345 - acc: 0.932 - ETA: 1s - loss: 0.2347 - acc: 0.932 - ETA: 1s - loss: 0.2355 - acc: 0.932 - ETA: 1s - loss: 0.2344 - acc: 0.932 - ETA: 1s - loss: 0.2339 - acc: 0.932 - ETA: 1s - loss: 0.2318 - acc: 0.932 - ETA: 1s - loss: 0.2321 - acc: 0.932 - ETA: 1s - loss: 0.2316 - acc: 0.932 - ETA: 1s - loss: 0.2313 - acc: 0.932 - ETA: 1s - loss: 0.2310 - acc: 0.933 - ETA: 1s - loss: 0.2302 - acc: 0.933 - ETA: 1s - loss: 0.2307 - acc: 0.932 - ETA: 1s - loss: 0.2313 - acc: 0.932 - ETA: 1s - loss: 0.2311 - acc: 0.932 - ETA: 1s - loss: 0.2315 - acc: 0.932 - ETA: 1s - loss: 0.2317 - acc: 0.932 - ETA: 1s - loss: 0.2319 - acc: 0.932 - ETA: 1s - loss: 0.2314 - acc: 0.932 - ETA: 1s - loss: 0.2303 - acc: 0.932 - ETA: 1s - loss: 0.2300 - acc: 0.932 - ETA: 1s - loss: 0.2295 - acc: 0.932 - ETA: 1s - loss: 0.2289 - acc: 0.933 - ETA: 0s - loss: 0.2278 - acc: 0.933 - ETA: 0s - loss: 0.2269 - acc: 0.933 - ETA: 0s - loss: 0.2272 - acc: 0.933 - ETA: 0s - loss: 0.2270 - acc: 0.933 - ETA: 0s - loss: 0.2271 - acc: 0.933 - ETA: 0s - loss: 0.2271 - acc: 0.933 - ETA: 0s - loss: 0.2262 - acc: 0.933 - ETA: 0s - loss: 0.2261 - acc: 0.933 - ETA: 0s - loss: 0.2257 - acc: 0.933 - ETA: 0s - loss: 0.2267 - acc: 0.933 - ETA: 0s - loss: 0.2264 - acc: 0.933 - ETA: 0s - loss: 0.2270 - acc: 0.933 - ETA: 0s - loss: 0.2263 - acc: 0.933 - ETA: 0s - loss: 0.2262 - acc: 0.933 - ETA: 0s - loss: 0.2265 - acc: 0.933 - ETA: 0s - loss: 0.2269 - acc: 0.933 - ETA: 0s - loss: 0.2275 - acc: 0.933 - ETA: 0s - loss: 0.2277 - acc: 0.933 - ETA: 0s - loss: 0.2277 - acc: 0.933 - ETA: 0s - loss: 0.2279 - acc: 0.933 - 4s 74us/step - loss: 0.2277 - acc: 0.9335 - val_loss: 0.2106 - val_acc: 0.9382\n",
      "Epoch 6/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - ETA: 10s - loss: 0.0714 - acc: 1.00 - ETA: 3s - loss: 0.1818 - acc: 0.9513 - ETA: 3s - loss: 0.2078 - acc: 0.938 - ETA: 3s - loss: 0.2028 - acc: 0.940 - ETA: 3s - loss: 0.2089 - acc: 0.939 - ETA: 3s - loss: 0.2064 - acc: 0.940 - ETA: 2s - loss: 0.2039 - acc: 0.941 - ETA: 2s - loss: 0.2131 - acc: 0.939 - ETA: 2s - loss: 0.2112 - acc: 0.938 - ETA: 2s - loss: 0.2103 - acc: 0.939 - ETA: 2s - loss: 0.2099 - acc: 0.940 - ETA: 2s - loss: 0.2104 - acc: 0.939 - ETA: 2s - loss: 0.2129 - acc: 0.938 - ETA: 2s - loss: 0.2114 - acc: 0.939 - ETA: 2s - loss: 0.2137 - acc: 0.938 - ETA: 2s - loss: 0.2107 - acc: 0.939 - ETA: 2s - loss: 0.2098 - acc: 0.940 - ETA: 2s - loss: 0.2093 - acc: 0.940 - ETA: 2s - loss: 0.2086 - acc: 0.941 - ETA: 2s - loss: 0.2080 - acc: 0.941 - ETA: 2s - loss: 0.2102 - acc: 0.939 - ETA: 2s - loss: 0.2109 - acc: 0.939 - ETA: 2s - loss: 0.2100 - acc: 0.939 - ETA: 2s - loss: 0.2099 - acc: 0.939 - ETA: 2s - loss: 0.2107 - acc: 0.939 - ETA: 2s - loss: 0.2107 - acc: 0.939 - ETA: 2s - loss: 0.2101 - acc: 0.940 - ETA: 2s - loss: 0.2092 - acc: 0.940 - ETA: 2s - loss: 0.2088 - acc: 0.939 - ETA: 2s - loss: 0.2090 - acc: 0.939 - ETA: 2s - loss: 0.2088 - acc: 0.939 - ETA: 1s - loss: 0.2093 - acc: 0.938 - ETA: 1s - loss: 0.2094 - acc: 0.938 - ETA: 1s - loss: 0.2083 - acc: 0.938 - ETA: 1s - loss: 0.2074 - acc: 0.939 - ETA: 1s - loss: 0.2074 - acc: 0.939 - ETA: 1s - loss: 0.2064 - acc: 0.939 - ETA: 1s - loss: 0.2061 - acc: 0.939 - ETA: 1s - loss: 0.2044 - acc: 0.939 - ETA: 1s - loss: 0.2045 - acc: 0.940 - ETA: 1s - loss: 0.2031 - acc: 0.940 - ETA: 1s - loss: 0.2032 - acc: 0.940 - ETA: 1s - loss: 0.2028 - acc: 0.940 - ETA: 1s - loss: 0.2032 - acc: 0.940 - ETA: 1s - loss: 0.2038 - acc: 0.940 - ETA: 1s - loss: 0.2054 - acc: 0.939 - ETA: 1s - loss: 0.2052 - acc: 0.939 - ETA: 1s - loss: 0.2057 - acc: 0.939 - ETA: 1s - loss: 0.2059 - acc: 0.939 - ETA: 1s - loss: 0.2061 - acc: 0.939 - ETA: 1s - loss: 0.2060 - acc: 0.939 - ETA: 1s - loss: 0.2059 - acc: 0.939 - ETA: 1s - loss: 0.2059 - acc: 0.939 - ETA: 1s - loss: 0.2055 - acc: 0.939 - ETA: 1s - loss: 0.2067 - acc: 0.939 - ETA: 1s - loss: 0.2063 - acc: 0.939 - ETA: 1s - loss: 0.2063 - acc: 0.939 - ETA: 1s - loss: 0.2059 - acc: 0.939 - ETA: 1s - loss: 0.2058 - acc: 0.939 - ETA: 1s - loss: 0.2058 - acc: 0.939 - ETA: 1s - loss: 0.2057 - acc: 0.939 - ETA: 1s - loss: 0.2055 - acc: 0.939 - ETA: 0s - loss: 0.2057 - acc: 0.939 - ETA: 0s - loss: 0.2063 - acc: 0.939 - ETA: 0s - loss: 0.2065 - acc: 0.939 - ETA: 0s - loss: 0.2066 - acc: 0.939 - ETA: 0s - loss: 0.2061 - acc: 0.939 - ETA: 0s - loss: 0.2062 - acc: 0.939 - ETA: 0s - loss: 0.2062 - acc: 0.939 - ETA: 0s - loss: 0.2062 - acc: 0.939 - ETA: 0s - loss: 0.2065 - acc: 0.939 - ETA: 0s - loss: 0.2068 - acc: 0.939 - ETA: 0s - loss: 0.2074 - acc: 0.938 - ETA: 0s - loss: 0.2076 - acc: 0.938 - ETA: 0s - loss: 0.2084 - acc: 0.938 - ETA: 0s - loss: 0.2092 - acc: 0.938 - ETA: 0s - loss: 0.2094 - acc: 0.938 - ETA: 0s - loss: 0.2087 - acc: 0.938 - ETA: 0s - loss: 0.2089 - acc: 0.938 - 4s 86us/step - loss: 0.2086 - acc: 0.9386 - val_loss: 0.1997 - val_acc: 0.9418\n",
      "Epoch 7/15\n",
      "50000/50000 [==============================] - ETA: 7s - loss: 0.2310 - acc: 0.937 - ETA: 5s - loss: 0.2294 - acc: 0.926 - ETA: 4s - loss: 0.2118 - acc: 0.934 - ETA: 4s - loss: 0.2156 - acc: 0.934 - ETA: 3s - loss: 0.2097 - acc: 0.936 - ETA: 3s - loss: 0.2041 - acc: 0.939 - ETA: 3s - loss: 0.1955 - acc: 0.940 - ETA: 3s - loss: 0.1981 - acc: 0.939 - ETA: 3s - loss: 0.1979 - acc: 0.939 - ETA: 2s - loss: 0.2025 - acc: 0.939 - ETA: 2s - loss: 0.2071 - acc: 0.938 - ETA: 2s - loss: 0.2062 - acc: 0.939 - ETA: 2s - loss: 0.2018 - acc: 0.940 - ETA: 3s - loss: 0.2016 - acc: 0.940 - ETA: 3s - loss: 0.2013 - acc: 0.940 - ETA: 3s - loss: 0.2010 - acc: 0.940 - ETA: 3s - loss: 0.2001 - acc: 0.941 - ETA: 3s - loss: 0.2011 - acc: 0.941 - ETA: 3s - loss: 0.2008 - acc: 0.941 - ETA: 3s - loss: 0.1982 - acc: 0.941 - ETA: 3s - loss: 0.1987 - acc: 0.941 - ETA: 3s - loss: 0.1972 - acc: 0.941 - ETA: 3s - loss: 0.1985 - acc: 0.941 - ETA: 3s - loss: 0.1987 - acc: 0.941 - ETA: 3s - loss: 0.1975 - acc: 0.942 - ETA: 3s - loss: 0.1991 - acc: 0.941 - ETA: 3s - loss: 0.1996 - acc: 0.941 - ETA: 3s - loss: 0.2000 - acc: 0.941 - ETA: 3s - loss: 0.2002 - acc: 0.942 - ETA: 2s - loss: 0.1989 - acc: 0.943 - ETA: 2s - loss: 0.1978 - acc: 0.943 - ETA: 2s - loss: 0.1985 - acc: 0.942 - ETA: 2s - loss: 0.1961 - acc: 0.943 - ETA: 2s - loss: 0.1955 - acc: 0.943 - ETA: 2s - loss: 0.1970 - acc: 0.943 - ETA: 2s - loss: 0.1971 - acc: 0.943 - ETA: 2s - loss: 0.1957 - acc: 0.943 - ETA: 2s - loss: 0.1959 - acc: 0.943 - ETA: 2s - loss: 0.1950 - acc: 0.943 - ETA: 2s - loss: 0.1944 - acc: 0.943 - ETA: 2s - loss: 0.1947 - acc: 0.943 - ETA: 2s - loss: 0.1949 - acc: 0.943 - ETA: 2s - loss: 0.1944 - acc: 0.944 - ETA: 1s - loss: 0.1941 - acc: 0.944 - ETA: 1s - loss: 0.1935 - acc: 0.944 - ETA: 1s - loss: 0.1923 - acc: 0.944 - ETA: 1s - loss: 0.1926 - acc: 0.944 - ETA: 1s - loss: 0.1927 - acc: 0.944 - ETA: 1s - loss: 0.1926 - acc: 0.944 - ETA: 1s - loss: 0.1936 - acc: 0.944 - ETA: 1s - loss: 0.1938 - acc: 0.944 - ETA: 1s - loss: 0.1941 - acc: 0.944 - ETA: 1s - loss: 0.1947 - acc: 0.943 - ETA: 1s - loss: 0.1945 - acc: 0.943 - ETA: 1s - loss: 0.1947 - acc: 0.943 - ETA: 1s - loss: 0.1947 - acc: 0.943 - ETA: 1s - loss: 0.1952 - acc: 0.943 - ETA: 1s - loss: 0.1955 - acc: 0.943 - ETA: 1s - loss: 0.1954 - acc: 0.943 - ETA: 1s - loss: 0.1952 - acc: 0.943 - ETA: 1s - loss: 0.1953 - acc: 0.943 - ETA: 1s - loss: 0.1953 - acc: 0.943 - ETA: 1s - loss: 0.1954 - acc: 0.943 - ETA: 1s - loss: 0.1955 - acc: 0.943 - ETA: 1s - loss: 0.1952 - acc: 0.943 - ETA: 1s - loss: 0.1951 - acc: 0.943 - ETA: 1s - loss: 0.1950 - acc: 0.943 - ETA: 1s - loss: 0.1950 - acc: 0.943 - ETA: 1s - loss: 0.1949 - acc: 0.943 - ETA: 1s - loss: 0.1946 - acc: 0.943 - ETA: 1s - loss: 0.1944 - acc: 0.943 - ETA: 0s - loss: 0.1941 - acc: 0.943 - ETA: 0s - loss: 0.1943 - acc: 0.943 - ETA: 0s - loss: 0.1944 - acc: 0.943 - ETA: 0s - loss: 0.1939 - acc: 0.943 - ETA: 0s - loss: 0.1942 - acc: 0.943 - ETA: 0s - loss: 0.1948 - acc: 0.943 - ETA: 0s - loss: 0.1952 - acc: 0.943 - ETA: 0s - loss: 0.1951 - acc: 0.943 - ETA: 0s - loss: 0.1952 - acc: 0.943 - ETA: 0s - loss: 0.1956 - acc: 0.942 - ETA: 0s - loss: 0.1956 - acc: 0.942 - ETA: 0s - loss: 0.1955 - acc: 0.942 - ETA: 0s - loss: 0.1960 - acc: 0.942 - ETA: 0s - loss: 0.1959 - acc: 0.942 - ETA: 0s - loss: 0.1956 - acc: 0.942 - ETA: 0s - loss: 0.1955 - acc: 0.943 - ETA: 0s - loss: 0.1950 - acc: 0.943 - ETA: 0s - loss: 0.1946 - acc: 0.943 - ETA: 0s - loss: 0.1941 - acc: 0.943 - ETA: 0s - loss: 0.1941 - acc: 0.943 - ETA: 0s - loss: 0.1937 - acc: 0.943 - ETA: 0s - loss: 0.1938 - acc: 0.943 - ETA: 0s - loss: 0.1936 - acc: 0.943 - ETA: 0s - loss: 0.1933 - acc: 0.943 - ETA: 0s - loss: 0.1931 - acc: 0.943 - ETA: 0s - loss: 0.1929 - acc: 0.943 - 6s 115us/step - loss: 0.1933 - acc: 0.9437 - val_loss: 0.1858 - val_acc: 0.9469\n",
      "Epoch 8/15\n",
      "50000/50000 [==============================] - ETA: 7s - loss: 0.0867 - acc: 0.968 - ETA: 5s - loss: 0.1467 - acc: 0.962 - ETA: 6s - loss: 0.1605 - acc: 0.951 - ETA: 6s - loss: 0.1556 - acc: 0.954 - ETA: 7s - loss: 0.1553 - acc: 0.952 - ETA: 8s - loss: 0.1541 - acc: 0.952 - ETA: 9s - loss: 0.1585 - acc: 0.950 - ETA: 10s - loss: 0.1618 - acc: 0.94 - ETA: 10s - loss: 0.1629 - acc: 0.94 - ETA: 10s - loss: 0.1659 - acc: 0.94 - ETA: 12s - loss: 0.1661 - acc: 0.94 - ETA: 10s - loss: 0.1796 - acc: 0.94 - ETA: 10s - loss: 0.1759 - acc: 0.94 - ETA: 9s - loss: 0.1784 - acc: 0.9465 - ETA: 9s - loss: 0.1770 - acc: 0.947 - ETA: 9s - loss: 0.1792 - acc: 0.947 - ETA: 8s - loss: 0.1797 - acc: 0.947 - ETA: 8s - loss: 0.1783 - acc: 0.947 - ETA: 7s - loss: 0.1797 - acc: 0.946 - ETA: 7s - loss: 0.1783 - acc: 0.947 - ETA: 6s - loss: 0.1836 - acc: 0.946 - ETA: 6s - loss: 0.1847 - acc: 0.944 - ETA: 5s - loss: 0.1821 - acc: 0.945 - ETA: 5s - loss: 0.1863 - acc: 0.943 - ETA: 5s - loss: 0.1898 - acc: 0.942 - ETA: 5s - loss: 0.1882 - acc: 0.943 - ETA: 4s - loss: 0.1888 - acc: 0.943 - ETA: 4s - loss: 0.1873 - acc: 0.943 - ETA: 4s - loss: 0.1846 - acc: 0.944 - ETA: 4s - loss: 0.1831 - acc: 0.944 - ETA: 4s - loss: 0.1829 - acc: 0.945 - ETA: 4s - loss: 0.1815 - acc: 0.945 - ETA: 3s - loss: 0.1823 - acc: 0.944 - ETA: 3s - loss: 0.1820 - acc: 0.945 - ETA: 3s - loss: 0.1825 - acc: 0.944 - ETA: 3s - loss: 0.1830 - acc: 0.944 - ETA: 3s - loss: 0.1837 - acc: 0.944 - ETA: 3s - loss: 0.1839 - acc: 0.944 - ETA: 3s - loss: 0.1840 - acc: 0.944 - ETA: 3s - loss: 0.1839 - acc: 0.944 - ETA: 3s - loss: 0.1827 - acc: 0.944 - ETA: 2s - loss: 0.1833 - acc: 0.944 - ETA: 2s - loss: 0.1836 - acc: 0.944 - ETA: 2s - loss: 0.1834 - acc: 0.944 - ETA: 2s - loss: 0.1837 - acc: 0.944 - ETA: 2s - loss: 0.1836 - acc: 0.944 - ETA: 2s - loss: 0.1829 - acc: 0.945 - ETA: 2s - loss: 0.1828 - acc: 0.945 - ETA: 2s - loss: 0.1833 - acc: 0.945 - ETA: 2s - loss: 0.1825 - acc: 0.945 - ETA: 2s - loss: 0.1821 - acc: 0.945 - ETA: 2s - loss: 0.1825 - acc: 0.945 - ETA: 2s - loss: 0.1821 - acc: 0.945 - ETA: 2s - loss: 0.1827 - acc: 0.945 - ETA: 2s - loss: 0.1822 - acc: 0.945 - ETA: 2s - loss: 0.1833 - acc: 0.945 - ETA: 2s - loss: 0.1831 - acc: 0.945 - ETA: 2s - loss: 0.1830 - acc: 0.945 - ETA: 2s - loss: 0.1832 - acc: 0.945 - ETA: 2s - loss: 0.1826 - acc: 0.945 - ETA: 2s - loss: 0.1832 - acc: 0.945 - ETA: 2s - loss: 0.1836 - acc: 0.945 - ETA: 2s - loss: 0.1843 - acc: 0.944 - ETA: 2s - loss: 0.1845 - acc: 0.944 - ETA: 2s - loss: 0.1842 - acc: 0.945 - ETA: 2s - loss: 0.1842 - acc: 0.944 - ETA: 2s - loss: 0.1841 - acc: 0.945 - ETA: 2s - loss: 0.1842 - acc: 0.945 - ETA: 2s - loss: 0.1844 - acc: 0.945 - ETA: 2s - loss: 0.1842 - acc: 0.945 - ETA: 2s - loss: 0.1837 - acc: 0.945 - ETA: 2s - loss: 0.1844 - acc: 0.945 - ETA: 2s - loss: 0.1842 - acc: 0.945 - ETA: 2s - loss: 0.1836 - acc: 0.945 - ETA: 2s - loss: 0.1830 - acc: 0.945 - ETA: 2s - loss: 0.1828 - acc: 0.945 - ETA: 2s - loss: 0.1827 - acc: 0.945 - ETA: 1s - loss: 0.1825 - acc: 0.945 - ETA: 1s - loss: 0.1822 - acc: 0.945 - ETA: 1s - loss: 0.1830 - acc: 0.945 - ETA: 1s - loss: 0.1828 - acc: 0.945 - ETA: 1s - loss: 0.1827 - acc: 0.945 - ETA: 1s - loss: 0.1823 - acc: 0.945 - ETA: 1s - loss: 0.1825 - acc: 0.945 - ETA: 1s - loss: 0.1829 - acc: 0.945 - ETA: 1s - loss: 0.1826 - acc: 0.945 - ETA: 1s - loss: 0.1825 - acc: 0.945 - ETA: 1s - loss: 0.1823 - acc: 0.945 - ETA: 1s - loss: 0.1825 - acc: 0.945 - ETA: 0s - loss: 0.1818 - acc: 0.945 - ETA: 0s - loss: 0.1816 - acc: 0.945 - ETA: 0s - loss: 0.1812 - acc: 0.945 - ETA: 0s - loss: 0.1813 - acc: 0.945 - ETA: 0s - loss: 0.1810 - acc: 0.945 - ETA: 0s - loss: 0.1808 - acc: 0.946 - ETA: 0s - loss: 0.1809 - acc: 0.945 - ETA: 0s - loss: 0.1811 - acc: 0.945 - ETA: 0s - loss: 0.1801 - acc: 0.946 - ETA: 0s - loss: 0.1799 - acc: 0.946 - ETA: 0s - loss: 0.1804 - acc: 0.946 - ETA: 0s - loss: 0.1804 - acc: 0.946 - ETA: 0s - loss: 0.1801 - acc: 0.946 - ETA: 0s - loss: 0.1800 - acc: 0.946 - ETA: 0s - loss: 0.1796 - acc: 0.946 - ETA: 0s - loss: 0.1806 - acc: 0.946 - ETA: 0s - loss: 0.1804 - acc: 0.946 - ETA: 0s - loss: 0.1803 - acc: 0.946 - 6s 119us/step - loss: 0.1804 - acc: 0.9462 - val_loss: 0.1743 - val_acc: 0.9494\n",
      "Epoch 9/15\n",
      "50000/50000 [==============================] - ETA: 9s - loss: 0.0764 - acc: 0.968 - ETA: 3s - loss: 0.1756 - acc: 0.938 - ETA: 3s - loss: 0.1519 - acc: 0.953 - ETA: 3s - loss: 0.1612 - acc: 0.953 - ETA: 2s - loss: 0.1676 - acc: 0.950 - ETA: 3s - loss: 0.1685 - acc: 0.951 - ETA: 3s - loss: 0.1663 - acc: 0.951 - ETA: 3s - loss: 0.1658 - acc: 0.951 - ETA: 3s - loss: 0.1678 - acc: 0.950 - ETA: 3s - loss: 0.1675 - acc: 0.951 - ETA: 3s - loss: 0.1677 - acc: 0.950 - ETA: 2s - loss: 0.1660 - acc: 0.951 - ETA: 2s - loss: 0.1672 - acc: 0.951 - ETA: 2s - loss: 0.1662 - acc: 0.952 - ETA: 2s - loss: 0.1661 - acc: 0.952 - ETA: 2s - loss: 0.1676 - acc: 0.952 - ETA: 2s - loss: 0.1667 - acc: 0.952 - ETA: 2s - loss: 0.1650 - acc: 0.953 - ETA: 2s - loss: 0.1651 - acc: 0.952 - ETA: 2s - loss: 0.1634 - acc: 0.952 - ETA: 2s - loss: 0.1636 - acc: 0.953 - ETA: 2s - loss: 0.1627 - acc: 0.953 - ETA: 2s - loss: 0.1625 - acc: 0.953 - ETA: 2s - loss: 0.1617 - acc: 0.953 - ETA: 2s - loss: 0.1617 - acc: 0.953 - ETA: 2s - loss: 0.1619 - acc: 0.953 - ETA: 2s - loss: 0.1631 - acc: 0.953 - ETA: 2s - loss: 0.1637 - acc: 0.953 - ETA: 2s - loss: 0.1637 - acc: 0.953 - ETA: 2s - loss: 0.1637 - acc: 0.953 - ETA: 2s - loss: 0.1638 - acc: 0.953 - ETA: 2s - loss: 0.1642 - acc: 0.953 - ETA: 2s - loss: 0.1652 - acc: 0.952 - ETA: 2s - loss: 0.1660 - acc: 0.952 - ETA: 2s - loss: 0.1666 - acc: 0.952 - ETA: 1s - loss: 0.1663 - acc: 0.952 - ETA: 1s - loss: 0.1667 - acc: 0.952 - ETA: 1s - loss: 0.1673 - acc: 0.951 - ETA: 1s - loss: 0.1679 - acc: 0.951 - ETA: 1s - loss: 0.1675 - acc: 0.951 - ETA: 1s - loss: 0.1668 - acc: 0.951 - ETA: 1s - loss: 0.1666 - acc: 0.951 - ETA: 1s - loss: 0.1663 - acc: 0.951 - ETA: 1s - loss: 0.1673 - acc: 0.951 - ETA: 1s - loss: 0.1681 - acc: 0.951 - ETA: 1s - loss: 0.1676 - acc: 0.951 - ETA: 1s - loss: 0.1675 - acc: 0.951 - ETA: 1s - loss: 0.1680 - acc: 0.951 - ETA: 1s - loss: 0.1681 - acc: 0.951 - ETA: 1s - loss: 0.1685 - acc: 0.951 - ETA: 1s - loss: 0.1686 - acc: 0.951 - ETA: 1s - loss: 0.1690 - acc: 0.951 - ETA: 1s - loss: 0.1694 - acc: 0.950 - ETA: 1s - loss: 0.1696 - acc: 0.950 - ETA: 1s - loss: 0.1689 - acc: 0.951 - ETA: 1s - loss: 0.1686 - acc: 0.951 - ETA: 1s - loss: 0.1691 - acc: 0.950 - ETA: 1s - loss: 0.1689 - acc: 0.950 - ETA: 0s - loss: 0.1689 - acc: 0.950 - ETA: 0s - loss: 0.1696 - acc: 0.950 - ETA: 0s - loss: 0.1688 - acc: 0.950 - ETA: 0s - loss: 0.1684 - acc: 0.950 - ETA: 0s - loss: 0.1681 - acc: 0.951 - ETA: 0s - loss: 0.1677 - acc: 0.951 - ETA: 0s - loss: 0.1680 - acc: 0.950 - ETA: 0s - loss: 0.1681 - acc: 0.950 - ETA: 0s - loss: 0.1675 - acc: 0.951 - ETA: 0s - loss: 0.1679 - acc: 0.950 - ETA: 0s - loss: 0.1680 - acc: 0.950 - ETA: 0s - loss: 0.1683 - acc: 0.950 - ETA: 0s - loss: 0.1679 - acc: 0.950 - ETA: 0s - loss: 0.1679 - acc: 0.950 - ETA: 0s - loss: 0.1680 - acc: 0.950 - ETA: 0s - loss: 0.1682 - acc: 0.950 - ETA: 0s - loss: 0.1680 - acc: 0.950 - ETA: 0s - loss: 0.1683 - acc: 0.950 - ETA: 0s - loss: 0.1681 - acc: 0.950 - ETA: 0s - loss: 0.1683 - acc: 0.950 - ETA: 0s - loss: 0.1683 - acc: 0.950 - ETA: 0s - loss: 0.1685 - acc: 0.950 - 4s 87us/step - loss: 0.1688 - acc: 0.9499 - val_loss: 0.1662 - val_acc: 0.9521\n",
      "Epoch 10/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - ETA: 10s - loss: 0.1008 - acc: 0.96 - ETA: 3s - loss: 0.1389 - acc: 0.9505 - ETA: 3s - loss: 0.1276 - acc: 0.957 - ETA: 3s - loss: 0.1370 - acc: 0.956 - ETA: 4s - loss: 0.1427 - acc: 0.956 - ETA: 3s - loss: 0.1376 - acc: 0.958 - ETA: 3s - loss: 0.1404 - acc: 0.957 - ETA: 3s - loss: 0.1455 - acc: 0.956 - ETA: 3s - loss: 0.1501 - acc: 0.954 - ETA: 3s - loss: 0.1506 - acc: 0.954 - ETA: 3s - loss: 0.1518 - acc: 0.954 - ETA: 3s - loss: 0.1502 - acc: 0.954 - ETA: 3s - loss: 0.1503 - acc: 0.954 - ETA: 3s - loss: 0.1514 - acc: 0.954 - ETA: 3s - loss: 0.1496 - acc: 0.955 - ETA: 3s - loss: 0.1506 - acc: 0.955 - ETA: 2s - loss: 0.1523 - acc: 0.955 - ETA: 2s - loss: 0.1554 - acc: 0.954 - ETA: 2s - loss: 0.1568 - acc: 0.954 - ETA: 2s - loss: 0.1566 - acc: 0.954 - ETA: 2s - loss: 0.1565 - acc: 0.954 - ETA: 2s - loss: 0.1562 - acc: 0.954 - ETA: 2s - loss: 0.1556 - acc: 0.954 - ETA: 2s - loss: 0.1558 - acc: 0.954 - ETA: 2s - loss: 0.1577 - acc: 0.953 - ETA: 2s - loss: 0.1572 - acc: 0.953 - ETA: 2s - loss: 0.1575 - acc: 0.953 - ETA: 2s - loss: 0.1584 - acc: 0.953 - ETA: 2s - loss: 0.1571 - acc: 0.953 - ETA: 2s - loss: 0.1571 - acc: 0.953 - ETA: 2s - loss: 0.1561 - acc: 0.953 - ETA: 2s - loss: 0.1567 - acc: 0.953 - ETA: 2s - loss: 0.1568 - acc: 0.953 - ETA: 2s - loss: 0.1565 - acc: 0.953 - ETA: 2s - loss: 0.1562 - acc: 0.953 - ETA: 2s - loss: 0.1575 - acc: 0.953 - ETA: 2s - loss: 0.1573 - acc: 0.953 - ETA: 2s - loss: 0.1583 - acc: 0.953 - ETA: 2s - loss: 0.1578 - acc: 0.953 - ETA: 2s - loss: 0.1575 - acc: 0.953 - ETA: 2s - loss: 0.1567 - acc: 0.954 - ETA: 2s - loss: 0.1567 - acc: 0.953 - ETA: 2s - loss: 0.1565 - acc: 0.953 - ETA: 2s - loss: 0.1558 - acc: 0.953 - ETA: 2s - loss: 0.1559 - acc: 0.954 - ETA: 1s - loss: 0.1552 - acc: 0.954 - ETA: 1s - loss: 0.1556 - acc: 0.954 - ETA: 1s - loss: 0.1561 - acc: 0.953 - ETA: 1s - loss: 0.1560 - acc: 0.953 - ETA: 1s - loss: 0.1561 - acc: 0.953 - ETA: 1s - loss: 0.1564 - acc: 0.953 - ETA: 1s - loss: 0.1566 - acc: 0.953 - ETA: 1s - loss: 0.1568 - acc: 0.953 - ETA: 1s - loss: 0.1564 - acc: 0.953 - ETA: 1s - loss: 0.1560 - acc: 0.953 - ETA: 1s - loss: 0.1559 - acc: 0.953 - ETA: 1s - loss: 0.1564 - acc: 0.953 - ETA: 1s - loss: 0.1569 - acc: 0.953 - ETA: 1s - loss: 0.1569 - acc: 0.953 - ETA: 1s - loss: 0.1572 - acc: 0.953 - ETA: 1s - loss: 0.1573 - acc: 0.952 - ETA: 1s - loss: 0.1573 - acc: 0.953 - ETA: 1s - loss: 0.1575 - acc: 0.953 - ETA: 1s - loss: 0.1575 - acc: 0.953 - ETA: 0s - loss: 0.1586 - acc: 0.952 - ETA: 0s - loss: 0.1586 - acc: 0.952 - ETA: 0s - loss: 0.1584 - acc: 0.952 - ETA: 0s - loss: 0.1584 - acc: 0.952 - ETA: 0s - loss: 0.1583 - acc: 0.952 - ETA: 0s - loss: 0.1586 - acc: 0.952 - ETA: 0s - loss: 0.1589 - acc: 0.952 - ETA: 0s - loss: 0.1587 - acc: 0.952 - ETA: 0s - loss: 0.1587 - acc: 0.952 - ETA: 0s - loss: 0.1588 - acc: 0.952 - ETA: 0s - loss: 0.1591 - acc: 0.952 - ETA: 0s - loss: 0.1590 - acc: 0.952 - ETA: 0s - loss: 0.1589 - acc: 0.952 - ETA: 0s - loss: 0.1588 - acc: 0.952 - ETA: 0s - loss: 0.1588 - acc: 0.952 - ETA: 0s - loss: 0.1587 - acc: 0.952 - ETA: 0s - loss: 0.1587 - acc: 0.952 - ETA: 0s - loss: 0.1590 - acc: 0.952 - ETA: 0s - loss: 0.1593 - acc: 0.952 - ETA: 0s - loss: 0.1594 - acc: 0.952 - ETA: 0s - loss: 0.1593 - acc: 0.952 - ETA: 0s - loss: 0.1596 - acc: 0.952 - ETA: 0s - loss: 0.1599 - acc: 0.952 - ETA: 0s - loss: 0.1597 - acc: 0.952 - ETA: 0s - loss: 0.1598 - acc: 0.952 - ETA: 0s - loss: 0.1596 - acc: 0.952 - ETA: 0s - loss: 0.1595 - acc: 0.952 - ETA: 0s - loss: 0.1597 - acc: 0.952 - ETA: 0s - loss: 0.1593 - acc: 0.952 - ETA: 0s - loss: 0.1590 - acc: 0.952 - ETA: 0s - loss: 0.1592 - acc: 0.952 - ETA: 0s - loss: 0.1589 - acc: 0.952 - ETA: 0s - loss: 0.1589 - acc: 0.952 - ETA: 0s - loss: 0.1592 - acc: 0.952 - 5s 109us/step - loss: 0.1593 - acc: 0.9526 - val_loss: 0.1631 - val_acc: 0.9527\n",
      "Epoch 11/15\n",
      "50000/50000 [==============================] - ETA: 10s - loss: 0.1947 - acc: 0.93 - ETA: 3s - loss: 0.2345 - acc: 0.9399 - ETA: 3s - loss: 0.2188 - acc: 0.944 - ETA: 3s - loss: 0.1994 - acc: 0.946 - ETA: 3s - loss: 0.1892 - acc: 0.949 - ETA: 3s - loss: 0.1854 - acc: 0.949 - ETA: 3s - loss: 0.1845 - acc: 0.948 - ETA: 3s - loss: 0.1774 - acc: 0.951 - ETA: 3s - loss: 0.1724 - acc: 0.951 - ETA: 3s - loss: 0.1724 - acc: 0.951 - ETA: 3s - loss: 0.1691 - acc: 0.951 - ETA: 3s - loss: 0.1672 - acc: 0.951 - ETA: 3s - loss: 0.1632 - acc: 0.952 - ETA: 3s - loss: 0.1620 - acc: 0.952 - ETA: 3s - loss: 0.1610 - acc: 0.952 - ETA: 3s - loss: 0.1602 - acc: 0.953 - ETA: 2s - loss: 0.1587 - acc: 0.953 - ETA: 2s - loss: 0.1570 - acc: 0.953 - ETA: 2s - loss: 0.1579 - acc: 0.953 - ETA: 2s - loss: 0.1587 - acc: 0.954 - ETA: 2s - loss: 0.1589 - acc: 0.954 - ETA: 2s - loss: 0.1592 - acc: 0.954 - ETA: 2s - loss: 0.1583 - acc: 0.954 - ETA: 2s - loss: 0.1575 - acc: 0.954 - ETA: 2s - loss: 0.1574 - acc: 0.954 - ETA: 2s - loss: 0.1571 - acc: 0.954 - ETA: 2s - loss: 0.1558 - acc: 0.954 - ETA: 2s - loss: 0.1560 - acc: 0.954 - ETA: 2s - loss: 0.1558 - acc: 0.954 - ETA: 2s - loss: 0.1561 - acc: 0.954 - ETA: 2s - loss: 0.1550 - acc: 0.954 - ETA: 2s - loss: 0.1548 - acc: 0.954 - ETA: 2s - loss: 0.1540 - acc: 0.955 - ETA: 2s - loss: 0.1539 - acc: 0.955 - ETA: 2s - loss: 0.1533 - acc: 0.955 - ETA: 2s - loss: 0.1526 - acc: 0.955 - ETA: 1s - loss: 0.1519 - acc: 0.955 - ETA: 1s - loss: 0.1523 - acc: 0.955 - ETA: 1s - loss: 0.1524 - acc: 0.955 - ETA: 1s - loss: 0.1511 - acc: 0.955 - ETA: 1s - loss: 0.1513 - acc: 0.955 - ETA: 1s - loss: 0.1509 - acc: 0.955 - ETA: 1s - loss: 0.1500 - acc: 0.955 - ETA: 1s - loss: 0.1504 - acc: 0.955 - ETA: 1s - loss: 0.1499 - acc: 0.955 - ETA: 1s - loss: 0.1503 - acc: 0.955 - ETA: 1s - loss: 0.1503 - acc: 0.955 - ETA: 1s - loss: 0.1496 - acc: 0.955 - ETA: 1s - loss: 0.1500 - acc: 0.955 - ETA: 1s - loss: 0.1502 - acc: 0.955 - ETA: 1s - loss: 0.1503 - acc: 0.956 - ETA: 1s - loss: 0.1505 - acc: 0.955 - ETA: 1s - loss: 0.1500 - acc: 0.956 - ETA: 1s - loss: 0.1500 - acc: 0.956 - ETA: 0s - loss: 0.1494 - acc: 0.956 - ETA: 0s - loss: 0.1492 - acc: 0.956 - ETA: 0s - loss: 0.1502 - acc: 0.956 - ETA: 0s - loss: 0.1507 - acc: 0.956 - ETA: 0s - loss: 0.1518 - acc: 0.955 - ETA: 0s - loss: 0.1514 - acc: 0.955 - ETA: 0s - loss: 0.1511 - acc: 0.955 - ETA: 0s - loss: 0.1512 - acc: 0.955 - ETA: 0s - loss: 0.1510 - acc: 0.956 - ETA: 0s - loss: 0.1510 - acc: 0.955 - ETA: 0s - loss: 0.1509 - acc: 0.955 - ETA: 0s - loss: 0.1508 - acc: 0.955 - ETA: 0s - loss: 0.1505 - acc: 0.955 - ETA: 0s - loss: 0.1505 - acc: 0.955 - ETA: 0s - loss: 0.1505 - acc: 0.955 - ETA: 0s - loss: 0.1505 - acc: 0.955 - ETA: 0s - loss: 0.1504 - acc: 0.955 - ETA: 0s - loss: 0.1504 - acc: 0.955 - ETA: 0s - loss: 0.1508 - acc: 0.955 - ETA: 0s - loss: 0.1507 - acc: 0.955 - 4s 80us/step - loss: 0.1504 - acc: 0.9556 - val_loss: 0.1559 - val_acc: 0.9551\n",
      "Epoch 12/15\n",
      "50000/50000 [==============================] - ETA: 10s - loss: 0.1548 - acc: 0.93 - ETA: 3s - loss: 0.1436 - acc: 0.9537 - ETA: 3s - loss: 0.1439 - acc: 0.958 - ETA: 3s - loss: 0.1363 - acc: 0.962 - ETA: 3s - loss: 0.1424 - acc: 0.960 - ETA: 3s - loss: 0.1438 - acc: 0.959 - ETA: 3s - loss: 0.1462 - acc: 0.958 - ETA: 3s - loss: 0.1443 - acc: 0.959 - ETA: 3s - loss: 0.1458 - acc: 0.959 - ETA: 3s - loss: 0.1444 - acc: 0.959 - ETA: 3s - loss: 0.1460 - acc: 0.959 - ETA: 3s - loss: 0.1478 - acc: 0.958 - ETA: 3s - loss: 0.1462 - acc: 0.959 - ETA: 3s - loss: 0.1439 - acc: 0.959 - ETA: 3s - loss: 0.1420 - acc: 0.960 - ETA: 3s - loss: 0.1432 - acc: 0.960 - ETA: 2s - loss: 0.1411 - acc: 0.960 - ETA: 2s - loss: 0.1410 - acc: 0.960 - ETA: 2s - loss: 0.1417 - acc: 0.960 - ETA: 2s - loss: 0.1437 - acc: 0.959 - ETA: 2s - loss: 0.1433 - acc: 0.959 - ETA: 2s - loss: 0.1420 - acc: 0.959 - ETA: 2s - loss: 0.1410 - acc: 0.959 - ETA: 2s - loss: 0.1409 - acc: 0.959 - ETA: 2s - loss: 0.1413 - acc: 0.959 - ETA: 2s - loss: 0.1407 - acc: 0.959 - ETA: 2s - loss: 0.1399 - acc: 0.959 - ETA: 2s - loss: 0.1393 - acc: 0.959 - ETA: 2s - loss: 0.1390 - acc: 0.959 - ETA: 2s - loss: 0.1393 - acc: 0.959 - ETA: 2s - loss: 0.1402 - acc: 0.959 - ETA: 2s - loss: 0.1400 - acc: 0.959 - ETA: 2s - loss: 0.1396 - acc: 0.959 - ETA: 2s - loss: 0.1403 - acc: 0.959 - ETA: 2s - loss: 0.1406 - acc: 0.959 - ETA: 2s - loss: 0.1417 - acc: 0.959 - ETA: 2s - loss: 0.1419 - acc: 0.958 - ETA: 1s - loss: 0.1412 - acc: 0.958 - ETA: 1s - loss: 0.1418 - acc: 0.958 - ETA: 1s - loss: 0.1413 - acc: 0.958 - ETA: 1s - loss: 0.1407 - acc: 0.958 - ETA: 1s - loss: 0.1404 - acc: 0.958 - ETA: 1s - loss: 0.1404 - acc: 0.958 - ETA: 1s - loss: 0.1403 - acc: 0.958 - ETA: 1s - loss: 0.1410 - acc: 0.958 - ETA: 1s - loss: 0.1406 - acc: 0.958 - ETA: 1s - loss: 0.1404 - acc: 0.958 - ETA: 1s - loss: 0.1408 - acc: 0.958 - ETA: 1s - loss: 0.1415 - acc: 0.958 - ETA: 1s - loss: 0.1417 - acc: 0.958 - ETA: 1s - loss: 0.1419 - acc: 0.958 - ETA: 1s - loss: 0.1425 - acc: 0.958 - ETA: 1s - loss: 0.1420 - acc: 0.958 - ETA: 1s - loss: 0.1418 - acc: 0.958 - ETA: 1s - loss: 0.1423 - acc: 0.958 - ETA: 1s - loss: 0.1419 - acc: 0.958 - ETA: 0s - loss: 0.1417 - acc: 0.958 - ETA: 0s - loss: 0.1414 - acc: 0.958 - ETA: 0s - loss: 0.1421 - acc: 0.958 - ETA: 0s - loss: 0.1429 - acc: 0.958 - ETA: 0s - loss: 0.1429 - acc: 0.958 - ETA: 0s - loss: 0.1430 - acc: 0.958 - ETA: 0s - loss: 0.1433 - acc: 0.958 - ETA: 0s - loss: 0.1433 - acc: 0.958 - ETA: 0s - loss: 0.1433 - acc: 0.957 - ETA: 0s - loss: 0.1433 - acc: 0.958 - ETA: 0s - loss: 0.1435 - acc: 0.958 - ETA: 0s - loss: 0.1434 - acc: 0.958 - ETA: 0s - loss: 0.1435 - acc: 0.958 - ETA: 0s - loss: 0.1431 - acc: 0.958 - ETA: 0s - loss: 0.1436 - acc: 0.958 - ETA: 0s - loss: 0.1439 - acc: 0.957 - ETA: 0s - loss: 0.1440 - acc: 0.957 - ETA: 0s - loss: 0.1440 - acc: 0.957 - ETA: 0s - loss: 0.1437 - acc: 0.957 - ETA: 0s - loss: 0.1437 - acc: 0.957 - ETA: 0s - loss: 0.1436 - acc: 0.958 - ETA: 0s - loss: 0.1433 - acc: 0.958 - ETA: 0s - loss: 0.1431 - acc: 0.958 - ETA: 0s - loss: 0.1432 - acc: 0.958 - ETA: 0s - loss: 0.1433 - acc: 0.958 - ETA: 0s - loss: 0.1435 - acc: 0.958 - ETA: 0s - loss: 0.1435 - acc: 0.958 - ETA: 0s - loss: 0.1430 - acc: 0.958 - ETA: 0s - loss: 0.1429 - acc: 0.958 - 5s 94us/step - loss: 0.1428 - acc: 0.9581 - val_loss: 0.1540 - val_acc: 0.9556\n",
      "Epoch 13/15\n",
      "50000/50000 [==============================] - ETA: 14s - loss: 0.2543 - acc: 0.87 - ETA: 3s - loss: 0.1558 - acc: 0.9440 - ETA: 6s - loss: 0.1641 - acc: 0.941 - ETA: 6s - loss: 0.1542 - acc: 0.947 - ETA: 6s - loss: 0.1382 - acc: 0.952 - ETA: 5s - loss: 0.1366 - acc: 0.955 - ETA: 4s - loss: 0.1425 - acc: 0.953 - ETA: 4s - loss: 0.1368 - acc: 0.955 - ETA: 4s - loss: 0.1344 - acc: 0.955 - ETA: 4s - loss: 0.1319 - acc: 0.956 - ETA: 4s - loss: 0.1296 - acc: 0.958 - ETA: 4s - loss: 0.1298 - acc: 0.958 - ETA: 4s - loss: 0.1405 - acc: 0.955 - ETA: 3s - loss: 0.1413 - acc: 0.955 - ETA: 3s - loss: 0.1393 - acc: 0.956 - ETA: 3s - loss: 0.1396 - acc: 0.956 - ETA: 3s - loss: 0.1394 - acc: 0.956 - ETA: 3s - loss: 0.1370 - acc: 0.957 - ETA: 3s - loss: 0.1359 - acc: 0.958 - ETA: 3s - loss: 0.1345 - acc: 0.959 - ETA: 3s - loss: 0.1359 - acc: 0.958 - ETA: 3s - loss: 0.1359 - acc: 0.958 - ETA: 2s - loss: 0.1354 - acc: 0.958 - ETA: 2s - loss: 0.1357 - acc: 0.957 - ETA: 2s - loss: 0.1350 - acc: 0.958 - ETA: 2s - loss: 0.1348 - acc: 0.958 - ETA: 2s - loss: 0.1349 - acc: 0.958 - ETA: 2s - loss: 0.1367 - acc: 0.958 - ETA: 2s - loss: 0.1375 - acc: 0.958 - ETA: 2s - loss: 0.1371 - acc: 0.958 - ETA: 2s - loss: 0.1390 - acc: 0.957 - ETA: 2s - loss: 0.1387 - acc: 0.957 - ETA: 2s - loss: 0.1395 - acc: 0.957 - ETA: 2s - loss: 0.1393 - acc: 0.957 - ETA: 2s - loss: 0.1393 - acc: 0.957 - ETA: 2s - loss: 0.1394 - acc: 0.957 - ETA: 2s - loss: 0.1394 - acc: 0.957 - ETA: 2s - loss: 0.1393 - acc: 0.957 - ETA: 2s - loss: 0.1393 - acc: 0.957 - ETA: 2s - loss: 0.1395 - acc: 0.957 - ETA: 2s - loss: 0.1402 - acc: 0.957 - ETA: 2s - loss: 0.1398 - acc: 0.957 - ETA: 2s - loss: 0.1395 - acc: 0.957 - ETA: 2s - loss: 0.1397 - acc: 0.957 - ETA: 2s - loss: 0.1393 - acc: 0.957 - ETA: 2s - loss: 0.1389 - acc: 0.957 - ETA: 2s - loss: 0.1379 - acc: 0.957 - ETA: 2s - loss: 0.1384 - acc: 0.957 - ETA: 2s - loss: 0.1383 - acc: 0.958 - ETA: 2s - loss: 0.1374 - acc: 0.958 - ETA: 1s - loss: 0.1370 - acc: 0.958 - ETA: 1s - loss: 0.1366 - acc: 0.958 - ETA: 1s - loss: 0.1365 - acc: 0.958 - ETA: 1s - loss: 0.1376 - acc: 0.958 - ETA: 1s - loss: 0.1372 - acc: 0.958 - ETA: 1s - loss: 0.1369 - acc: 0.958 - ETA: 1s - loss: 0.1369 - acc: 0.958 - ETA: 1s - loss: 0.1365 - acc: 0.959 - ETA: 1s - loss: 0.1366 - acc: 0.958 - ETA: 1s - loss: 0.1364 - acc: 0.958 - ETA: 1s - loss: 0.1357 - acc: 0.959 - ETA: 1s - loss: 0.1360 - acc: 0.959 - ETA: 1s - loss: 0.1362 - acc: 0.959 - ETA: 1s - loss: 0.1366 - acc: 0.958 - ETA: 1s - loss: 0.1360 - acc: 0.959 - ETA: 1s - loss: 0.1361 - acc: 0.959 - ETA: 0s - loss: 0.1360 - acc: 0.959 - ETA: 0s - loss: 0.1360 - acc: 0.959 - ETA: 0s - loss: 0.1364 - acc: 0.959 - ETA: 0s - loss: 0.1371 - acc: 0.958 - ETA: 0s - loss: 0.1367 - acc: 0.959 - ETA: 0s - loss: 0.1369 - acc: 0.959 - ETA: 0s - loss: 0.1371 - acc: 0.958 - ETA: 0s - loss: 0.1371 - acc: 0.958 - ETA: 0s - loss: 0.1367 - acc: 0.959 - ETA: 0s - loss: 0.1366 - acc: 0.959 - ETA: 0s - loss: 0.1364 - acc: 0.959 - ETA: 0s - loss: 0.1362 - acc: 0.959 - ETA: 0s - loss: 0.1363 - acc: 0.959 - ETA: 0s - loss: 0.1357 - acc: 0.959 - ETA: 0s - loss: 0.1357 - acc: 0.959 - ETA: 0s - loss: 0.1354 - acc: 0.959 - ETA: 0s - loss: 0.1357 - acc: 0.959 - 5s 90us/step - loss: 0.1355 - acc: 0.9595 - val_loss: 0.1469 - val_acc: 0.9580\n",
      "Epoch 14/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - ETA: 10s - loss: 0.1053 - acc: 0.96 - ETA: 3s - loss: 0.1196 - acc: 0.9625 - ETA: 3s - loss: 0.1245 - acc: 0.959 - ETA: 3s - loss: 0.1195 - acc: 0.960 - ETA: 3s - loss: 0.1234 - acc: 0.961 - ETA: 3s - loss: 0.1316 - acc: 0.958 - ETA: 4s - loss: 0.1263 - acc: 0.961 - ETA: 4s - loss: 0.1254 - acc: 0.961 - ETA: 4s - loss: 0.1276 - acc: 0.960 - ETA: 4s - loss: 0.1245 - acc: 0.962 - ETA: 4s - loss: 0.1288 - acc: 0.962 - ETA: 4s - loss: 0.1257 - acc: 0.962 - ETA: 4s - loss: 0.1271 - acc: 0.962 - ETA: 4s - loss: 0.1277 - acc: 0.961 - ETA: 4s - loss: 0.1272 - acc: 0.962 - ETA: 4s - loss: 0.1264 - acc: 0.962 - ETA: 3s - loss: 0.1275 - acc: 0.962 - ETA: 3s - loss: 0.1255 - acc: 0.962 - ETA: 3s - loss: 0.1284 - acc: 0.961 - ETA: 3s - loss: 0.1290 - acc: 0.961 - ETA: 3s - loss: 0.1312 - acc: 0.960 - ETA: 3s - loss: 0.1316 - acc: 0.960 - ETA: 3s - loss: 0.1327 - acc: 0.960 - ETA: 3s - loss: 0.1318 - acc: 0.960 - ETA: 3s - loss: 0.1321 - acc: 0.960 - ETA: 2s - loss: 0.1301 - acc: 0.961 - ETA: 2s - loss: 0.1297 - acc: 0.961 - ETA: 2s - loss: 0.1298 - acc: 0.961 - ETA: 2s - loss: 0.1296 - acc: 0.961 - ETA: 2s - loss: 0.1303 - acc: 0.961 - ETA: 2s - loss: 0.1298 - acc: 0.961 - ETA: 2s - loss: 0.1296 - acc: 0.961 - ETA: 2s - loss: 0.1288 - acc: 0.962 - ETA: 2s - loss: 0.1297 - acc: 0.961 - ETA: 2s - loss: 0.1295 - acc: 0.961 - ETA: 1s - loss: 0.1304 - acc: 0.961 - ETA: 1s - loss: 0.1302 - acc: 0.961 - ETA: 1s - loss: 0.1306 - acc: 0.961 - ETA: 1s - loss: 0.1299 - acc: 0.961 - ETA: 1s - loss: 0.1307 - acc: 0.961 - ETA: 1s - loss: 0.1307 - acc: 0.961 - ETA: 1s - loss: 0.1309 - acc: 0.961 - ETA: 1s - loss: 0.1305 - acc: 0.961 - ETA: 1s - loss: 0.1300 - acc: 0.961 - ETA: 1s - loss: 0.1302 - acc: 0.961 - ETA: 1s - loss: 0.1292 - acc: 0.961 - ETA: 1s - loss: 0.1290 - acc: 0.961 - ETA: 1s - loss: 0.1296 - acc: 0.961 - ETA: 1s - loss: 0.1294 - acc: 0.961 - ETA: 1s - loss: 0.1293 - acc: 0.961 - ETA: 1s - loss: 0.1292 - acc: 0.961 - ETA: 1s - loss: 0.1294 - acc: 0.961 - ETA: 1s - loss: 0.1295 - acc: 0.961 - ETA: 1s - loss: 0.1294 - acc: 0.961 - ETA: 1s - loss: 0.1295 - acc: 0.961 - ETA: 0s - loss: 0.1295 - acc: 0.961 - ETA: 0s - loss: 0.1288 - acc: 0.961 - ETA: 0s - loss: 0.1288 - acc: 0.961 - ETA: 0s - loss: 0.1289 - acc: 0.961 - ETA: 0s - loss: 0.1290 - acc: 0.961 - ETA: 0s - loss: 0.1285 - acc: 0.961 - ETA: 0s - loss: 0.1291 - acc: 0.961 - ETA: 0s - loss: 0.1299 - acc: 0.961 - ETA: 0s - loss: 0.1304 - acc: 0.961 - ETA: 0s - loss: 0.1300 - acc: 0.961 - ETA: 0s - loss: 0.1298 - acc: 0.961 - ETA: 0s - loss: 0.1294 - acc: 0.961 - ETA: 0s - loss: 0.1295 - acc: 0.961 - ETA: 0s - loss: 0.1296 - acc: 0.961 - ETA: 0s - loss: 0.1298 - acc: 0.961 - ETA: 0s - loss: 0.1297 - acc: 0.961 - 4s 77us/step - loss: 0.1293 - acc: 0.9614 - val_loss: 0.1399 - val_acc: 0.9603\n",
      "Epoch 15/15\n",
      "50000/50000 [==============================] - ETA: 12s - loss: 0.0734 - acc: 0.96 - ETA: 3s - loss: 0.1137 - acc: 0.9632 - ETA: 2s - loss: 0.1094 - acc: 0.965 - ETA: 2s - loss: 0.1084 - acc: 0.965 - ETA: 2s - loss: 0.1132 - acc: 0.966 - ETA: 2s - loss: 0.1153 - acc: 0.965 - ETA: 2s - loss: 0.1216 - acc: 0.964 - ETA: 2s - loss: 0.1237 - acc: 0.963 - ETA: 2s - loss: 0.1244 - acc: 0.963 - ETA: 2s - loss: 0.1249 - acc: 0.963 - ETA: 2s - loss: 0.1264 - acc: 0.962 - ETA: 2s - loss: 0.1256 - acc: 0.962 - ETA: 2s - loss: 0.1270 - acc: 0.962 - ETA: 2s - loss: 0.1267 - acc: 0.962 - ETA: 2s - loss: 0.1264 - acc: 0.962 - ETA: 2s - loss: 0.1260 - acc: 0.961 - ETA: 2s - loss: 0.1253 - acc: 0.962 - ETA: 2s - loss: 0.1245 - acc: 0.962 - ETA: 2s - loss: 0.1241 - acc: 0.963 - ETA: 2s - loss: 0.1250 - acc: 0.962 - ETA: 2s - loss: 0.1246 - acc: 0.963 - ETA: 2s - loss: 0.1250 - acc: 0.962 - ETA: 2s - loss: 0.1250 - acc: 0.962 - ETA: 2s - loss: 0.1257 - acc: 0.962 - ETA: 2s - loss: 0.1251 - acc: 0.963 - ETA: 2s - loss: 0.1247 - acc: 0.963 - ETA: 2s - loss: 0.1254 - acc: 0.963 - ETA: 1s - loss: 0.1259 - acc: 0.962 - ETA: 1s - loss: 0.1259 - acc: 0.962 - ETA: 1s - loss: 0.1261 - acc: 0.962 - ETA: 1s - loss: 0.1261 - acc: 0.962 - ETA: 1s - loss: 0.1260 - acc: 0.962 - ETA: 1s - loss: 0.1248 - acc: 0.962 - ETA: 1s - loss: 0.1246 - acc: 0.962 - ETA: 1s - loss: 0.1249 - acc: 0.962 - ETA: 1s - loss: 0.1254 - acc: 0.962 - ETA: 1s - loss: 0.1250 - acc: 0.962 - ETA: 1s - loss: 0.1248 - acc: 0.962 - ETA: 1s - loss: 0.1246 - acc: 0.962 - ETA: 1s - loss: 0.1248 - acc: 0.962 - ETA: 1s - loss: 0.1247 - acc: 0.963 - ETA: 1s - loss: 0.1242 - acc: 0.963 - ETA: 1s - loss: 0.1239 - acc: 0.963 - ETA: 1s - loss: 0.1238 - acc: 0.963 - ETA: 1s - loss: 0.1239 - acc: 0.963 - ETA: 1s - loss: 0.1234 - acc: 0.963 - ETA: 1s - loss: 0.1239 - acc: 0.963 - ETA: 0s - loss: 0.1242 - acc: 0.963 - ETA: 0s - loss: 0.1235 - acc: 0.963 - ETA: 0s - loss: 0.1240 - acc: 0.963 - ETA: 0s - loss: 0.1239 - acc: 0.963 - ETA: 0s - loss: 0.1236 - acc: 0.963 - ETA: 0s - loss: 0.1237 - acc: 0.963 - ETA: 0s - loss: 0.1235 - acc: 0.963 - ETA: 0s - loss: 0.1232 - acc: 0.963 - ETA: 0s - loss: 0.1228 - acc: 0.963 - ETA: 0s - loss: 0.1231 - acc: 0.963 - ETA: 0s - loss: 0.1230 - acc: 0.963 - ETA: 0s - loss: 0.1234 - acc: 0.963 - ETA: 0s - loss: 0.1235 - acc: 0.963 - ETA: 0s - loss: 0.1232 - acc: 0.963 - ETA: 0s - loss: 0.1229 - acc: 0.963 - ETA: 0s - loss: 0.1230 - acc: 0.963 - ETA: 0s - loss: 0.1236 - acc: 0.963 - ETA: 0s - loss: 0.1237 - acc: 0.963 - ETA: 0s - loss: 0.1238 - acc: 0.963 - 4s 72us/step - loss: 0.1239 - acc: 0.9634 - val_loss: 0.1400 - val_acc: 0.9591\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21bc200d668>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(hidden_layers_size, activation=\"relu\"))\n",
    "model.add(Dense(hidden_layers_size, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=SGD(lr=0.01),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=15,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
