{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на данные: это отзывы о ресторанах и оценка. Будем решать многоклассовую классификацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id\tSentiment\tText\r\n",
      "0\t1\tIncredibly disappointing service. I mean really, really bad.\\n\\nWe placed an order for delivery at 6:30 pm on a Tuesday night, not the busiest night of the week, I'm sure. We were given an estimate of 30-40 minutes. After an hour my husband called to make sure our order wasn't forgotten. The young girl on the phone said that they were very busy and the driver was on his way to our house (less than a mile from the restaurant) at that time and should arrive in 10 minutes. After another 30 minutes we called back and asked to please cancel the order, after 1 1/2 hours we no longer wanted the food. The girl on the phone shouted at my husband that none of this was her fault and was reluctant to cancel our order. She wanted to charge us for food we never received!\\n\\nThe food is just not good enough for such poor service. If 18 year old college students can't answers phones and take simple orders don't hire them. It's simple.\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 2 train.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  102583 train.data\r\n"
     ]
    }
   ],
   "source": [
    "! wc -l train.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем выборку, поделим на трейн и тест так, чтобы в x_train был raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = csv.reader(open('train.data'), delimiter='\\t')\n",
    "next(train_file)\n",
    "train_set = [x for x in train_file]\n",
    "\n",
    "train_data, train_label = [line[2] for line in train_set], [line[1] for line in train_set]\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "x_train, x_validate, y_train, y_validate = train_test_split(train_data, train_label, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I like this location because they have a drive-thru. Even though there is almost always a long line, they get you on your way fast. The staff is friendly and competent. Also, they rarely run out of anything (other locations seem to go through their entire inventory of breakfast sandwiches and scones by 9am).\\\\n\\\\nIf you are the type that does not drink your morning coffee inside a moving vehicle, they also have comfy chairs inside and decent patio seating.  The patio faces the parking lot and drive-thru but it does have shade umbrellas so it can be very pleasant in the morning.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Тупое\" решение:\n",
    "\n",
    "Посмотрим что будет, если применить самое простое решение: найти 100 самых частотных слов и использовать их в качестве признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def create_bow_with_freq(data):\n",
    "    result = Counter()\n",
    "    for s in data:\n",
    "        result.update(s.strip().split())\n",
    "    return list(result.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of unique \"words\": ', 484082)\n"
     ]
    }
   ],
   "source": [
    "train_bow = create_bow_with_freq(x_train)\n",
    "print('Number of unique \"words\": ', len(train_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 654951),\n",
       " ('and', 492240),\n",
       " ('a', 411657),\n",
       " ('I', 385802),\n",
       " ('to', 359227),\n",
       " ('of', 248340),\n",
       " ('was', 240102),\n",
       " ('is', 184703),\n",
       " ('for', 167194),\n",
       " ('in', 162966)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_frequent_word = sorted(train_bow, key=lambda x: x[1], reverse=True)[:100]\n",
    "most_frequent_word[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_bow_sample(bow, sample):\n",
    "    for s in sample:\n",
    "        s = s.strip().split()\n",
    "        yield { word:word in s for word, _ in bow}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow_train = [(x, y) for x, y in zip(make_bow_sample(most_frequent_word, x_train), y_train)]\n",
    "bow_validate = [x for x in make_bow_sample(most_frequent_word, x_validate)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'-': False,\n",
       "  'I': True,\n",
       "  \"I'm\": False,\n",
       "  \"I've\": False,\n",
       "  'It': False,\n",
       "  'My': False,\n",
       "  'The': True,\n",
       "  'They': False,\n",
       "  'This': False,\n",
       "  'We': False,\n",
       "  'a': True,\n",
       "  'about': False,\n",
       "  'all': False,\n",
       "  'also': True,\n",
       "  'always': True,\n",
       "  'an': False,\n",
       "  'and': True,\n",
       "  'are': True,\n",
       "  'as': False,\n",
       "  'at': False,\n",
       "  'back': False,\n",
       "  'be': True,\n",
       "  'because': True,\n",
       "  'been': False,\n",
       "  'but': True,\n",
       "  'by': True,\n",
       "  'can': True,\n",
       "  'could': False,\n",
       "  \"didn't\": False,\n",
       "  'do': False,\n",
       "  \"don't\": False,\n",
       "  'even': False,\n",
       "  'food': False,\n",
       "  'for': False,\n",
       "  'from': False,\n",
       "  'get': True,\n",
       "  'go': True,\n",
       "  'good': False,\n",
       "  'got': False,\n",
       "  'great': False,\n",
       "  'had': False,\n",
       "  'has': False,\n",
       "  'have': True,\n",
       "  'he': False,\n",
       "  'here': False,\n",
       "  'if': False,\n",
       "  'in': True,\n",
       "  'is': True,\n",
       "  'it': True,\n",
       "  \"it's\": False,\n",
       "  'just': False,\n",
       "  'like': True,\n",
       "  'little': False,\n",
       "  'love': False,\n",
       "  'me': False,\n",
       "  'more': False,\n",
       "  'much': False,\n",
       "  'my': False,\n",
       "  'nice': False,\n",
       "  'no': False,\n",
       "  'not': True,\n",
       "  'of': True,\n",
       "  'on': True,\n",
       "  'one': False,\n",
       "  'only': False,\n",
       "  'or': False,\n",
       "  'ordered': False,\n",
       "  'other': False,\n",
       "  'our': False,\n",
       "  'out': True,\n",
       "  'place': False,\n",
       "  'pretty': False,\n",
       "  'really': False,\n",
       "  'service': False,\n",
       "  'so': True,\n",
       "  'some': False,\n",
       "  'than': False,\n",
       "  'that': True,\n",
       "  'the': True,\n",
       "  'their': True,\n",
       "  'them': False,\n",
       "  'there': True,\n",
       "  'they': True,\n",
       "  'this': True,\n",
       "  'time': False,\n",
       "  'to': True,\n",
       "  'up': False,\n",
       "  'us': False,\n",
       "  'very': True,\n",
       "  'was': False,\n",
       "  'we': False,\n",
       "  'were': False,\n",
       "  'what': False,\n",
       "  'when': False,\n",
       "  'which': False,\n",
       "  'will': False,\n",
       "  'with': False,\n",
       "  'would': False,\n",
       "  'you': True,\n",
       "  'your': True},\n",
       " '3')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся наивным байесовским классификатором. \n",
    "Плюс данного классификатора - можно посмотреть какиме слова оказались наиболее полезными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                    love = True                5 : 1      =      3.5 : 1.0\n",
      "                   great = True                5 : 1      =      3.1 : 1.0\n",
      "                     was = False               5 : 1      =      2.7 : 1.0\n",
      "                  always = True                5 : 1      =      2.6 : 1.0\n",
      "                  pretty = True                3 : 1      =      2.5 : 1.0\n",
      "                      no = True                1 : 5      =      2.4 : 1.0\n",
      "                      he = True                1 : 4      =      2.4 : 1.0\n",
      "                      to = False               4 : 1      =      2.3 : 1.0\n",
      "                  didn't = True                2 : 5      =      2.3 : 1.0\n",
      "                    nice = True                4 : 1      =      2.2 : 1.0\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = nltk.NaiveBayesClassifier.train(bow_train)\n",
    "print(nb.show_most_informative_features())\n",
    "predicted = [nb.classify(o) for o in bow_validate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20509,), (20509,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(map(float,predicted)).shape, np.array(y_validate).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.3787605441513482\n"
     ]
    }
   ],
   "source": [
    "print 'accuracy', np.mean(np.array(map(float,predicted))== np.array(map(float, y_validate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = map(float, y_train)\n",
    "y_validate = map(float, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nkozlovskaya/anaconda2/lib/python2.7/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(encoding=u'utf-8', ngram_range=(1, 2), analyzer='word')\n",
    "Xtrain = tfidf.fit_transform(x_train)\n",
    "Xtest = tfidf.transform(x_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Попробуйте LodisticRegression, LinearSVC, SGDClassifier с какой-нибудь функцией потерь.\n",
    "- При обучении  SGDClassifier не забудьте поставить побольше итераций, так как это итеративный метод\n",
    "- параметр class_weight='balanced' может быть полезен. Что он означает?\n",
    "- Можете повариьировать так же параметры TF-IDF vectorizer\n",
    "- Попробовать прологарифмировать частоты, или другое нелинейное преобразование."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM vs LinearSVC\n",
    "(LinearSVC быстрее, но не выадет вероятностей, а лишь расстояние до решающей границы. Перевести в вероятности можно, откалибровав)\n",
    "\n",
    "Кроме того для достаижения качества полезно логарифмировать np.log1p()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C=1, random_state=3,  n_jobs=-1)\n",
    "lr.fit(Xtrain, y_train)\n",
    "lr_pr = lr.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5904724754985616\n"
     ]
    }
   ],
   "source": [
    "print 'accuracy', np.mean(np.array(map(float,lr_pr))== np.array(map(float, y_validate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clf = [LogisticRegression(n_jobs=-1)\n",
    "# LinearSVC(C=1, loss='hinge', class_weight='balanced'),\n",
    "# SGDClassifier(loss='modified_huber', class_weight='balanced', alpha=1e-2, n_iter=50, n_jobs=-1),\n",
    "# SGDClassifier(loss='squared_hinge', class_weight='balanced', alpha=1e-2, n_iter=50, n_jobs=-1),\n",
    "# SGDClassifier(loss='hinge',class_weight='balanced', alpha=1e-2, random_state=3, n_iter=50, n_jobs=-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какой алгоритм сработал лучше свего?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда обучаем многоклассовую классификацию для такой задачи, не учитываем то, что метки 1 и 2 более похожи между собой, чем 4 и 5. Как это можно было бы учесть при обучении модели?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переходим к нейросетевым подходам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "архитектуры отсюда"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from keras.utils import np_utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXT_LENGTH = 100\n",
    "VOCABULARY_SIZE = 50000\n",
    "EMBEDDING_DIM = 30\n",
    "DIMS = 56\n",
    "MAX_FEATURES = 5000\n",
    "batch_size = 32\n",
    "\n",
    "nb_filter = 50\n",
    "filter_length = 3\n",
    "hidden_dims = 50\n",
    "nb_epoch = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I like this location because they have a drive-thru. Even though there is almost always a long line, they get you on your way fast. The staff is friendly and competent. Also, they rarely run out of anything (other locations seem to go through their entire inventory of breakfast sandwiches and scones by 9am).\\\\n\\\\nIf you are the type that does not drink your morning coffee inside a moving vehicle, they also have comfy chairs inside and decent patio seating.  The patio faces the parking lot and drive-thru but it does have shade umbrellas so it can be very pleasant in the morning.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.text.Tokenizer at 0x113f64610>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "X_train = tokenizer.sequences_to_matrix(sequences, mode='count')\n",
    "sequences = tokenizer.texts_to_sequences(x_validate)\n",
    "X_test = tokenizer.sequences_to_matrix(sequences, mode='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((82035, 5000), (20509, 5000))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train)[:, 1:]\n",
    "y_test = np_utils.to_categorical(y_validate)[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 73831 samples, validate on 8204 samples\n",
      "Epoch 1/3\n",
      "73831/73831 [==============================] - 28s 384us/step - loss: 1.0169 - acc: 0.5529 - val_loss: 0.9552 - val_acc: 0.5709\n",
      "Epoch 2/3\n",
      "73831/73831 [==============================] - 26s 351us/step - loss: 0.8811 - acc: 0.6150 - val_loss: 0.9642 - val_acc: 0.5755\n",
      "Epoch 3/3\n",
      "73831/73831 [==============================] - 24s 330us/step - loss: 0.7943 - acc: 0.6555 - val_loss: 1.0009 - val_acc: 0.5657\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1290a0cd0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(MAX_FEATURES,), activation = 'relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(5, activation = 'softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=nb_epoch, batch_size=batch_size,  validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5656053439953191"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = model.predict(X_test)\n",
    "np.mean(np.argmax(pr, axis=1) == np.argmax(y_test, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь пробуем LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "X_train = sequence.pad_sequences(sequences, maxlen=TEXT_LENGTH)\n",
    "sequences = tokenizer.texts_to_sequences(x_validate)\n",
    "X_test = sequence.pad_sequences(sequences, maxlen=TEXT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_review_length = max([len(el) for el in X_train])\n",
    "top_words = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 100, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 213,705\n",
      "Trainable params: 213,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 82035 samples, validate on 20509 samples\n",
      "Epoch 1/3\n",
      "82035/82035 [==============================] - 239s 3ms/step - loss: 0.4109 - acc: 0.8104 - val_loss: 0.3651 - val_acc: 0.8250\n",
      "Epoch 2/3\n",
      "82035/82035 [==============================] - 238s 3ms/step - loss: 0.3503 - acc: 0.8313 - val_loss: 0.3522 - val_acc: 0.8299\n",
      "Epoch 3/3\n",
      "82035/82035 [==============================] - 232s 3ms/step - loss: 0.3359 - acc: 0.8388 - val_loss: 0.3494 - val_acc: 0.8291\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x133122ed0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding_vecor_length = 32\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(50000, embedding_vecor_length, input_length=500))\n",
    "# model.add(LSTM(100))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# print(model.summary())\n",
    "# model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)\n",
    "\n",
    "\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(5, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5473694475596079"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = model.predict(X_test)\n",
    "np.mean(np.argmax(pr, axis=1) == np.argmax(y_test, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавляем сверточный слой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
