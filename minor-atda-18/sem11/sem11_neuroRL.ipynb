{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Приближенные методы к вычислению Q-функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с определения агента, который обучается, используя Q-функцию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, math\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class QLearningAgent():\n",
    "    \"\"\"\n",
    "    Q-Learning Agent\n",
    "\n",
    "    Instance variables you have access to\n",
    "      - self.epsilon (exploration prob)\n",
    "      - self.alpha (learning rate)\n",
    "      - self.discount (discount rate aka gamma)\n",
    "\n",
    "    Functions you should use\n",
    "      - self.getLegalActions(state)\n",
    "        which returns legal actions for a state\n",
    "      - self.getQValue(state,action)\n",
    "        which returns Q(state,action)\n",
    "      - self.setQValue(state,action,value)\n",
    "        which sets Q(state,action) := value\n",
    "\n",
    "    !!!Important!!!\n",
    "    NOTE: please avoid using self._qValues directly to make code cleaner\n",
    "    \"\"\"\n",
    "    def __init__(self,alpha,epsilon,discount,getLegalActions):\n",
    "        \"We initialize agent and Q-values here.\"\n",
    "        self.getLegalActions= getLegalActions\n",
    "        self._qValues = defaultdict(lambda:defaultdict(lambda:0))\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.discount = discount\n",
    "\n",
    "    def getQValue(self, state, action):\n",
    "        \"\"\"\n",
    "          Returns Q(state,action)\n",
    "        \"\"\"\n",
    "        return self._qValues[state][action]\n",
    "\n",
    "    def setQValue(self,state,action,value):\n",
    "        \"\"\"\n",
    "          Sets the Qvalue for [state,action] to the given value\n",
    "        \"\"\"\n",
    "        self._qValues[state][action] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим нашему агенту возможность вычислять оценки $V$: $V_{(i+1)}(s) = \\max_a Q_i(s,a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getValue(self, state):\n",
    "    \"\"\"\n",
    "      Returns max_action Q(state,action)\n",
    "      where the max is over legal actions.\n",
    "    \"\"\"\n",
    "    \n",
    "    possibleActions = self.getLegalActions(state)\n",
    "    #If there are no legal actions, return 0.0\n",
    "    if len(possibleActions) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # ДАЛЬШЕ ВАШ КОД\n",
    "    value = None\n",
    "    return value\n",
    "\n",
    "QLearningAgent.getValue =  getValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стратегия нашего агента будет заключаться в выборе лучшего действия, в соответствии с оценками $Q$: $a = argmax_a Q(s,a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPolicy(self, state):\n",
    "    \"\"\"\n",
    "      Compute the best action to take in a state. \n",
    "      \n",
    "    \"\"\"\n",
    "    possibleActions = self.getLegalActions(state)\n",
    "\n",
    "    #If there are no legal actions, return None\n",
    "    if len(possibleActions) == 0:\n",
    "        return None\n",
    "    \n",
    "    best_action = None\n",
    "    # ДАЛЬШЕ ВАШ КОД\n",
    "    \n",
    "    return best_action\n",
    "\n",
    "QLearningAgent.getPolicy = getPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для конкретной ситуации мы будем выбирать действие, используя $\\epsilon$-жадный подход:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAction(self, state):\n",
    "    \"\"\"\n",
    "      Compute the action to take in the current state, including exploration.  \n",
    "      \n",
    "      With probability self.epsilon, we should take a random action.\n",
    "      otherwise - the best policy action (self.getPolicy).\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pick Action\n",
    "    possibleActions = self.getLegalActions(state)\n",
    "\n",
    "    action = None\n",
    "    #If there are no legal actions, return None\n",
    "    if len(possibleActions) == 0:\n",
    "        return action\n",
    "\n",
    "    # ДАЛЬШЕ ВАШ КОД\n",
    "    \n",
    "    return action\n",
    "\n",
    "QLearningAgent.getAction = getAction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция обнолвления оценки $Q$: $Q_i(s, a) = r(s,a,s') + \\gamma V_{i}(s')$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(self, state, action, nextState, reward):\n",
    "    \"\"\"\n",
    "      You should do your Q-Value update here\n",
    "\n",
    "      NOTE: You should never call this function,\n",
    "      it will be called on your behalf\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "   \n",
    "    # ДАЛЬШЕ ВАШ КОД\n",
    "    reference_qvalue = None\n",
    "    \n",
    "    updated_qvalue = (1 - self.alpha) * self.getQValue(state, action) + self.alpha * reference_qvalue\n",
    "    self.setQValue(state, action, updated_qvalue)\n",
    "    \n",
    "QLearningAgent.update = update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тестируем нашего агента на задаче Taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QLearningAgent(alpha=0.5, epsilon=0.1, discount=0.99,\n",
    "                       getLegalActions = lambda s: range(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'getPolicy' in dir(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_train(env, agent, t_max=10**4):\n",
    "    \"\"\"This function should \n",
    "    - run a full game, actions given by agent.getAction(s)\n",
    "    - train agent using agent.update(...) whenever possible\n",
    "    - return total reward\"\"\"\n",
    "    total_reward = 0.0\n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        # ДАЛЬШЕ ВАШ КОД\n",
    "        a = None\n",
    "        \n",
    "        next_s, r, done, _ = env.step(a)\n",
    "        \n",
    "        # ДАЛЬШЕ ВАШ КОД\n",
    "        # обучение агента\n",
    "        \n",
    "        s = next_s\n",
    "        total_reward +=r\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "\n",
    "agent =  None\n",
    "# Создаем своего агента с парамметрами alpha=0.5, epsilon=0.1, discount=0.99\n",
    "\n",
    "rewards = []\n",
    "for i in range(1000):\n",
    "    rewards.append(play_and_train(env, agent))\n",
    "    \n",
    "    agent.epsilon *= 0.999\n",
    "    \n",
    "    if i % 100 ==0:\n",
    "        clear_output(True)\n",
    "        print(agent.epsilon)\n",
    "        plt.plot(rewards)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Будем приближенно считать значение оценок $Q$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](qlearning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать классическую задачу с непрерывным набором состояний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\").env\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import keras.layers as L\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем небольшую сетку - два скрытых слоя со 100 нейронами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "network = Sequential()\n",
    "network.add(L.InputLayer(state_dim))\n",
    "\n",
    "# ДАЛЬШЕ ВАШ КОД по созданию сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_action(state, epsilon=0):\n",
    "    \"\"\"\n",
    "    sample actions with epsilon-greedy policy\n",
    "    recap: with p = epsilon pick random action, else pick action with highest Q(s,a)\n",
    "    \"\"\"\n",
    "    \n",
    "    q_values = network.predict(state[None])[0]\n",
    "    \n",
    "    action = None\n",
    "    # ДАЛЬШЕ ВАШ КОД\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=0.0 tests passed\n",
      "e=0.1 tests passed\n",
      "e=0.5 tests passed\n",
      "e=1.0 tests passed\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import random\n",
    "\n",
    "assert network.output_shape == (None, n_actions), \"please make sure your model maps state s -> [Q(s,a0), ..., Q(s, a_last)]\"\n",
    "assert network.layers[-1].activation == keras.activations.linear, \"please make sure you predict q-values without nonlinearity\"\n",
    "\n",
    "# test epsilon-greedy exploration\n",
    "s = env.reset()\n",
    "assert np.shape(get_action(s)) == (), \"please return just one action (integer)\"\n",
    "for eps in [0., 0.1, 0.5, 1.0]:\n",
    "    state_frequencies = np.bincount([get_action(s, epsilon=eps) for i in range(10000)], minlength=n_actions)\n",
    "    best_action = state_frequencies.argmax()\n",
    "    assert abs(state_frequencies[best_action] - 10000 * (1 - eps + eps / n_actions)) < 200\n",
    "    for other_action in range(n_actions):\n",
    "        if other_action != best_action:\n",
    "            assert abs(state_frequencies[other_action] - 10000 * (eps / n_actions)) < 200\n",
    "    print('e=%.1f tests passed'%eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Будем обучать нашу сеть, используя так называемую функцию потерь TD (time-difference):\n",
    "\n",
    "$$ L = { 1 \\over N} \\sum_i (Q_{\\theta}(s,a) - [r(s,a) + \\gamma \\cdot max_{a'} Q_{-}(s', a')]) ^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хитрость здесь заключается в том, что мы используем \"отложенное\" состояние сети $Q_{-}(s',a')$, которое не изменяется в процессе распространения ошибки по текущему состоянию сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create placeholders for the <s, a, r, s'> tuple and a special indicator for game end (is_done = True)\n",
    "states_ph = tf.placeholder('float32', shape=(None,) + state_dim)\n",
    "actions_ph = tf.placeholder('int32', shape=[None])\n",
    "rewards_ph = tf.placeholder('float32', shape=[None])\n",
    "next_states_ph = tf.placeholder('float32', shape=(None,) + state_dim)\n",
    "is_done_ph = tf.placeholder('bool', shape=[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get q-values for all actions in current states\n",
    "predicted_qvalues = network(states_ph)\n",
    "\n",
    "#select q-values for chosen actions\n",
    "predicted_qvalues_for_actions = tf.reduce_sum(predicted_qvalues * tf.one_hot(actions_ph, n_actions), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "# compute q-values for all actions in next states\n",
    "# ДАЛЬШЕ ВАШ КОД\n",
    "predicted_next_qvalues = None\n",
    "\n",
    "# compute V*(next_states) using predicted next q-values\n",
    "# ДАЛЬШЕ ВАШ КОД\n",
    "next_state_values = None\n",
    "\n",
    "# compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
    "# ДАЛЬШЕ ВАШ КОД\n",
    "target_qvalues_for_actions = None\n",
    "\n",
    "# at the last state we shall use simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "target_qvalues_for_actions = tf.where(is_done_ph, rewards_ph, target_qvalues_for_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean squared error loss to minimize\n",
    "loss = (predicted_qvalues_for_actions - tf.stop_gradient(target_qvalues_for_actions)) ** 2\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "# training function that resembles agent.update(state, action, reward, next_state) from tabular agent\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tf.gradients(loss, [predicted_qvalues_for_actions])[0] is not None, \"make sure you update q-values for chosen actions and not just all actions\"\n",
    "assert tf.gradients(loss, [predicted_next_qvalues])[0] is None, \"make sure you don't propagate gradient w.r.t. Q_(s',a')\"\n",
    "assert predicted_next_qvalues.shape.ndims == 2, \"make sure you predicted q-values for all actions in next state\"\n",
    "assert next_state_values.shape.ndims == 1, \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n",
    "assert target_qvalues_for_actions.shape.ndims == 1, \"there's something wrong with target q-values, they must be a vector\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь проверим как работает наш нейроагент"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000, epsilon=0, train=False):\n",
    "    \"\"\"play env with approximate q-learning agent and train it at the same time\"\"\"\n",
    "    total_reward = 0\n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        a = get_action(s, epsilon=epsilon)       \n",
    "        next_s, r, done, _ = env.step(a)\n",
    "        \n",
    "        if train:\n",
    "            sess.run(train_step,{\n",
    "                states_ph: [s], actions_ph: [a], rewards_ph: [r], \n",
    "                next_states_ph: [next_s], is_done_ph: [done]\n",
    "            })\n",
    "\n",
    "        total_reward += r\n",
    "        s = next_s\n",
    "        if done: break\n",
    "            \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.5\n",
    "for i in range(1000):\n",
    "    session_rewards = [generate_session(epsilon=epsilon, train=True) for _ in range(100)]\n",
    "    print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(i, np.mean(session_rewards), epsilon))\n",
    "    \n",
    "    epsilon *= 0.99\n",
    "    assert epsilon >= 1e-4, \"Make sure epsilon is always nonzero during training\"\n",
    "    \n",
    "    if np.mean(session_rewards) > 300:\n",
    "        print (\"You Win!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
