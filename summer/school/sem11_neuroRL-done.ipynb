{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Приближенные методы к вычислению Q-функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с определения агента, который обучается, используя Q-функцию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, math\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class QLearningAgent():\n",
    "    \"\"\"\n",
    "    Q-Learning Agent\n",
    "\n",
    "    Instance variables you have access to\n",
    "      - self.epsilon (exploration prob)\n",
    "      - self.alpha (learning rate)\n",
    "      - self.discount (discount rate aka gamma)\n",
    "\n",
    "    Functions you should use\n",
    "      - self.getLegalActions(state)\n",
    "        which returns legal actions for a state\n",
    "      - self.getQValue(state,action)\n",
    "        which returns Q(state,action)\n",
    "      - self.setQValue(state,action,value)\n",
    "        which sets Q(state,action) := value\n",
    "\n",
    "    !!!Important!!!\n",
    "    NOTE: please avoid using self._qValues directly to make code cleaner\n",
    "    \"\"\"\n",
    "    def __init__(self,alpha,epsilon,discount,getLegalActions):\n",
    "        \"We initialize agent and Q-values here.\"\n",
    "        self.getLegalActions= getLegalActions\n",
    "        self._qValues = defaultdict(lambda:defaultdict(lambda:0))\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.discount = discount\n",
    "\n",
    "    def getQValue(self, state, action):\n",
    "        \"\"\"\n",
    "          Returns Q(state,action)\n",
    "        \"\"\"\n",
    "        return self._qValues[state][action]\n",
    "\n",
    "    def setQValue(self,state,action,value):\n",
    "        \"\"\"\n",
    "          Sets the Qvalue for [state,action] to the given value\n",
    "        \"\"\"\n",
    "        self._qValues[state][action] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим нашему агенту возможность вычислять оценки $V$: $V_{(i+1)}(s) = \\max_a Q_i(s,a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getValue(self, state):\n",
    "    \"\"\"\n",
    "      Returns max_action Q(state,action)\n",
    "      where the max is over legal actions.\n",
    "    \"\"\"\n",
    "    \n",
    "    possibleActions = self.getLegalActions(state)\n",
    "    #If there are no legal actions, return 0.0\n",
    "    if len(possibleActions) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    value = max([self._qValues[state][action] for action in possibleActions])\n",
    "    return value\n",
    "\n",
    "QLearningAgent.getValue =  getValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стратегия нашего агента будет заключаться в выборе лучшего действия, в соответствии с оценками $Q$: $a = argmax_a Q(s,a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPolicy(self, state):\n",
    "    \"\"\"\n",
    "      Compute the best action to take in a state. \n",
    "      \n",
    "    \"\"\"\n",
    "    possibleActions = self.getLegalActions(state)\n",
    "\n",
    "    #If there are no legal actions, return None\n",
    "    if len(possibleActions) == 0:\n",
    "        return None\n",
    "    \n",
    "    best_action = None\n",
    "    best_q = 0\n",
    "    for action in possibleActions:\n",
    "        q = self._qValues[state][action]\n",
    "        if best_action is None or best_q < q:\n",
    "            best_action = action\n",
    "            best_q = q\n",
    "    \n",
    "    return best_action\n",
    "\n",
    "QLearningAgent.getPolicy = getPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для конкретной ситуации мы будем выбирать действие, используя $\\epsilon$-жадный подход:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAction(self, state):\n",
    "    \"\"\"\n",
    "      Compute the action to take in the current state, including exploration.  \n",
    "      \n",
    "      With probability self.epsilon, we should take a random action.\n",
    "      otherwise - the best policy action (self.getPolicy).\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pick Action\n",
    "    possibleActions = self.getLegalActions(state)\n",
    "\n",
    "    action = None\n",
    "    #If there are no legal actions, return None\n",
    "    if len(possibleActions) == 0:\n",
    "        return action\n",
    "\n",
    "    if self.epsilon > random.random():\n",
    "        action = random.choice(possibleActions)\n",
    "    else:\n",
    "        action = self.getPolicy(state)\n",
    "    \n",
    "    return action\n",
    "\n",
    "QLearningAgent.getAction = getAction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция обнолвления оценки $Q$: $Q_i(s, a) = r(s,a,s') + \\gamma V_{i}(s')$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(self, state, action, nextState, reward):\n",
    "    \"\"\"\n",
    "      You should do your Q-Value update here\n",
    "\n",
    "      NOTE: You should never call this function,\n",
    "      it will be called on your behalf\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "   \n",
    "    #reference_qvalue = <the \"correct state value\", uses reward and the value of next state>\n",
    "    reference_qvalue = reward + self.discount * self.getValue(nextState)\n",
    "    \n",
    "    updated_qvalue = (1 - self.alpha) * self.getQValue(state, action) + self.alpha * reference_qvalue\n",
    "    self.setQValue(state, action, updated_qvalue)\n",
    "    \n",
    "QLearningAgent.update = update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тестируем нашего агента на задаче Taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QLearningAgent(alpha=0.5, epsilon=0.1, discount=0.99,\n",
    "                       getLegalActions = lambda s: range(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'getPolicy' in dir(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_train(env, agent, t_max=10**4):\n",
    "    \"\"\"This function should \n",
    "    - run a full game, actions given by agent.getAction(s)\n",
    "    - train agent using agent.update(...) whenever possible\n",
    "    - return total reward\"\"\"\n",
    "    total_reward = 0.0\n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        #a = <get agent to pick action given state s>\n",
    "        a = agent.getAction(s)\n",
    "        \n",
    "        next_s, r, done, _ = env.step(a)\n",
    "        \n",
    "        #<train(update) agent for state s>\n",
    "        agent.update(s, a, next_s, r)\n",
    "        \n",
    "        s = next_s\n",
    "        total_reward +=r\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04059802359226594\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1756b2e5f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "\n",
    "agent = QLearningAgent(alpha=0.5, epsilon=0.1, discount=0.99,\n",
    "                       getLegalActions = lambda s: range(n_actions))\n",
    "\n",
    "rewards = []\n",
    "for i in range(1000):\n",
    "    rewards.append(play_and_train(env, agent))\n",
    "    \n",
    "    agent.epsilon *= 0.999\n",
    "    \n",
    "    if i % 100 ==0:\n",
    "        clear_output(True)\n",
    "        print(agent.epsilon)\n",
    "        plt.plot(rewards)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Будем приближенно считать значение оценок $Q$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](qlearning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать классическую задачу с непрерывным набором состояний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d69b01c518>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAEoRJREFUeJzt3X+s3fV93/Hna5hAlmQ1hAvybDOT1ltDp8WwO0LENFFIW2DRTKVmgk0NipAuk4iUqNFW6KQ1kYbUSmvYom2obqFxpiyEkWRYiDVlDlGVPwIxiUMwDuUmseJbe/hmAZIsGhvkvT/O54Yz+/je4/vD1+fD8yEdne/38/2c73l/8OF1v/7c78cnVYUkqT9/Zb0LkCStDQNekjplwEtSpwx4SeqUAS9JnTLgJalTaxbwSa5L8myS2SR3rNX7SJJGy1rcB5/kLOAvgF8B5oCvAjdX1TOr/maSpJHW6gr+CmC2qr5TVf8HuB/YuUbvJUkaYcManXczcHhofw5458k6X3DBBbVt27Y1KkWSJs+hQ4f4/ve/n5WcY60CflRR/99cUJIZYAbg4osvZt++fWtUiiRNnunp6RWfY62maOaArUP7W4Ajwx2qaldVTVfV9NTU1BqVIUmvX2sV8F8Ftie5JMkbgJuAPWv0XpKkEdZkiqaqXknyAeALwFnAfVV1YC3eS5I02lrNwVNVjwCPrNX5JUmLcyWrJHXKgJekThnwktQpA16SOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTBrwkdcqAl6ROregr+5IcAn4EvAq8UlXTSc4HPgNsAw4B/7iqXlhZmZKkU7UaV/C/XFU7qmq67d8B7K2q7cDeti9JOs3WYopmJ7C7be8GblyD95AkLWGlAV/AnyV5MslMa7uoqo4CtOcLV/gekqRlWNEcPHBVVR1JciHwaJJvjfvC9gNhBuDiiy9eYRmSpOOt6Aq+qo6052PA54ErgOeTbAJoz8dO8tpdVTVdVdNTU1MrKUOSNMKyAz7Jm5K8ZWEb+FXgaWAPcEvrdgvw0EqLlCSdupVM0VwEfD7Jwnn+c1X9aZKvAg8kuRX4HvDelZcpSTpVyw74qvoO8I4R7f8TuHYlRUmSVs6VrJLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTBrwkdcqAl6ROGfCS1CkDXpI6ZcBLUqcMeEnqlAEvSZ0y4CWpUwa8JHXKgJekThnwktQpA16SOmXAS1Knlgz4JPclOZbk6aG285M8muS59nxea0+SjyeZTfJUksvXsnhJ0smNcwX/CeC649ruAPZW1XZgb9sHuB7Y3h4zwD2rU6Yk6VQtGfBV9efAD45r3gnsbtu7gRuH2j9ZA18BNibZtFrFSpLGt9w5+Iuq6ihAe76wtW8GDg/1m2ttJ0gyk2Rfkn3z8/PLLEOSdDKr/UvWjGirUR2raldVTVfV9NTU1CqXIUlabsA/vzD10p6PtfY5YOtQvy3AkeWXJ0laruUG/B7glrZ9C/DQUPv72t00VwIvLUzlSJJOrw1LdUjyaeBq4IIkc8DvAr8HPJDkVuB7wHtb90eAG4BZ4CfA+9egZknSGJYM+Kq6+SSHrh3Rt4DbV1qUJGnlXMkqSZ0y4CWpUwa8JHXKgJekThnwktQpA16SOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTSwZ8kvuSHEvy9FDbR5L8ZZL97XHD0LE7k8wmeTbJr61V4ZKkxY1zBf8J4LoR7XdX1Y72eAQgyaXATcAvtdf8xyRnrVaxkqTxLRnwVfXnwA/GPN9O4P6qermqvgvMAlesoD5J0jKtZA7+A0mealM457W2zcDhoT5zre0ESWaS7Euyb35+fgVlSJJGWW7A3wP8PLADOAr8QWvPiL416gRVtauqpqtqempqapllSJJOZlkBX1XPV9WrVfVT4I94bRpmDtg61HULcGRlJUqSlmNZAZ9k09DurwMLd9jsAW5Kck6SS4DtwBMrK1GStBwbluqQ5NPA1cAFSeaA3wWuTrKDwfTLIeA2gKo6kOQB4BngFeD2qnp1bUqXJC1myYCvqptHNN+7SP+7gLtWUpQkaeVcySpJnTLgJalTBrwkdcqAl6ROGfCS1CkDXpI6teRtktLr1ZO7bjuh7e/O/OE6VCItj1fw0gijwl2aNAa8JHXKgJekThnwktQpA16SOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4tGfBJtiZ5LMnBJAeSfLC1n5/k0STPtefzWnuSfDzJbJKnkly+1oOQJJ1onCv4V4APV9XbgSuB25NcCtwB7K2q7cDetg9wPbC9PWaAe1a9aknSkpYM+Ko6WlVfa9s/Ag4Cm4GdwO7WbTdwY9veCXyyBr4CbEyyadUrlyQt6pTm4JNsAy4DHgcuqqqjMPghAFzYum0GDg+9bK61HX+umST7kuybn58/9colSYsaO+CTvBn4LPChqvrhYl1HtNUJDVW7qmq6qqanpqbGLUOSNKaxAj7J2QzC/VNV9bnW/PzC1Et7Ptba54CtQy/fAhxZnXIlSeMa5y6aAPcCB6vqY0OH9gC3tO1bgIeG2t/X7qa5EnhpYSpHknT6jPOVfVcBvwl8M8n+1vY7wO8BDyS5Ffge8N527BHgBmAW+Anw/lWtWFonfl2fJs2SAV9VX2b0vDrAtSP6F3D7CuuSJK2QK1klqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTBrwkdcqAl6ROGfCS1CkDXpI6ZcBLUqcMeEnqlAEvSZ0y4KXjPLnrthPa/DYnTSIDXpI6Nc6Xbm9N8liSg0kOJPlga/9Ikr9Msr89bhh6zZ1JZpM8m+TX1nIAkqTRxvnS7VeAD1fV15K8BXgyyaPt2N1V9W+GOye5FLgJ+CXgrwP/PcnfrKpXV7NwSdLilryCr6qjVfW1tv0j4CCweZGX7ATur6qXq+q7wCxwxWoUK0ka3ynNwSfZBlwGPN6aPpDkqST3JTmvtW0GDg+9bI7FfyBIktbA2AGf5M3AZ4EPVdUPgXuAnwd2AEeBP1joOuLlNeJ8M0n2Jdk3Pz9/yoVLkhY3VsAnOZtBuH+qqj4HUFXPV9WrVfVT4I94bRpmDtg69PItwJHjz1lVu6pquqqmp6amVjIGSdII49xFE+Be4GBVfWyofdNQt18Hnm7be4CbkpyT5BJgO/DE6pUsSRrHOHfRXAX8JvDNJPtb2+8ANyfZwWD65RBwG0BVHUjyAPAMgztwbvcOGkk6/ZYM+Kr6MqPn1R9Z5DV3AXetoC5J0gq5klWSOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8NKQUd/mJE0qA16SOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvF4Xkoz1WOnrFzuHdLoZ8JLUqXG+8EN63Xn46MzPtt+zadc6ViItnwEvDRkOdmnSOUUjLcHQ16Qa50u3z03yRJJvJDmQ5KOt/ZIkjyd5LslnkryhtZ/T9mfb8W1rOwRpbTlFo0k1zhX8y8A1VfUOYAdwXZIrgd8H7q6q7cALwK2t/63AC1X1C8DdrZ80Ed6zaZeBrm6M86XbBfy47Z7dHgVcA/yT1r4b+AhwD7CzbQM8CPz7JGnnkc5o07cthPtrIf+RdalEWrmx5uCTnJVkP3AMeBT4NvBiVb3SuswBm9v2ZuAwQDv+EvDW1SxakrS0sQK+ql6tqh3AFuAK4O2jurXnUSs9Trh6TzKTZF+SffPz8+PWK0ka0yndRVNVLwJfAq4ENiZZmOLZAhxp23PAVoB2/OeAH4w4166qmq6q6ampqeVVL0k6qXHuoplKsrFtvxF4N3AQeAz4jdbtFuChtr2n7dOOf9H5d0k6/cZZ6LQJ2J3kLAY/EB6oqoeTPAPcn+RfA18H7m397wX+U5JZBlfuN61B3ZKkJYxzF81TwGUj2r/DYD7++Pb/Dbx3VaqTJC2bK1klqVMGvCR1yoCXpE75r0nqdcEbufR65BW8JHXKgJekThnwktQpA16SOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SerUOF+6fW6SJ5J8I8mBJB9t7Z9I8t0k+9tjR2tPko8nmU3yVJLL13oQkqQTjfPvwb8MXFNVP05yNvDlJP+tHfvnVfXgcf2vB7a3xzuBe9qzJOk0WvIKvgZ+3HbPbo/Fvj1hJ/DJ9rqvABuTbFp5qZKkUzHWHHySs5LsB44Bj1bV4+3QXW0a5u4k57S2zcDhoZfPtTZJ0mk0VsBX1atVtQPYAlyR5G8DdwK/CPw94Hzgt1v3jDrF8Q1JZpLsS7Jvfn5+WcVLkk7ulO6iqaoXgS8B11XV0TYN8zLwJ8AVrdscsHXoZVuAIyPOtauqpqtqempqalnFS5JObpy7aKaSbGzbbwTeDXxrYV49SYAbgafbS/YA72t301wJvFRVR9ekeknSSY1zF80mYHeSsxj8QHigqh5O8sUkUwymZPYD/6z1fwS4AZgFfgK8f/XLliQtZcmAr6qngMtGtF9zkv4F3L7y0iRJK+FKVknqlAEvSZ0y4CWpUwa8JHXKgJekThnwktQpA16SOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTYwd8krOSfD3Jw23/kiSPJ3kuyWeSvKG1n9P2Z9vxbWtTuiRpMadyBf9B4ODQ/u8Dd1fVduAF4NbWfivwQlX9AnB36ydJOs3GCvgkW4B/CPxx2w9wDfBg67IbuLFt72z7tOPXtv6SpNNow5j9/i3wL4C3tP23Ai9W1Sttfw7Y3LY3A4cBquqVJC+1/t8fPmGSGWCm7b6c5OlljeDMdwHHjb0TvY4L+h2b45osfyPJTFXtWu4Jlgz4JO8BjlXVk0muXmge0bXGOPZaw6DoXe099lXV9FgVT5hex9bruKDfsTmuyZNkHy0nl2OcK/irgH+U5AbgXOCvMbii35hkQ7uK3wIcaf3ngK3AXJINwM8BP1hugZKk5VlyDr6q7qyqLVW1DbgJ+GJV/VPgMeA3WrdbgIfa9p62Tzv+xao64QpekrS2VnIf/G8Dv5VklsEc+72t/V7gra39t4A7xjjXsv8KMgF6HVuv44J+x+a4Js+KxhYvriWpT65klaROrXvAJ7kuybNt5es40zlnlCT3JTk2fJtnkvOTPNpW+T6a5LzWniQfb2N9Ksnl61f54pJsTfJYkoNJDiT5YGuf6LElOTfJE0m+0cb10dbexcrsXlecJzmU5JtJ9rc7Syb+swiQZGOSB5N8q/2/9q7VHNe6BnySs4D/AFwPXArcnOTS9axpGT4BXHdc2x3A3rbKdy+v/R7iemB7e8wA95ymGpfjFeDDVfV24Erg9vZnM+ljexm4pqreAewArktyJf2szO55xfkvV9WOoVsiJ/2zCPDvgD+tql8E3sHgz271xlVV6/YA3gV8YWj/TuDO9axpmePYBjw9tP8ssKltbwKebdt/CNw8qt+Z/mBwl9Sv9DQ24K8CXwPeyWChzIbW/rPPJfAF4F1te0Prl/Wu/STj2dIC4RrgYQZrUiZ+XK3GQ8AFx7VN9GeRwS3n3z3+v/tqjmu9p2h+tuq1GV4RO8kuqqqjAO35wtY+keNtf32/DHicDsbWpjH2A8eAR4FvM+bKbGBhZfaZaGHF+U/b/tgrzjmzxwWDxZJ/luTJtgoeJv+z+DZgHviTNq32x0nexCqOa70DfqxVrx2ZuPEmeTPwWeBDVfXDxbqOaDsjx1ZVr1bVDgZXvFcAbx/VrT1PxLgytOJ8uHlE14ka15CrqupyBtMUtyf5B4v0nZSxbQAuB+6pqsuA/8Xit5Wf8rjWO+AXVr0uGF4RO8meT7IJoD0fa+0TNd4kZzMI909V1edacxdjA6iqF4EvMfgdw8a28hpGr8zmDF+ZvbDi/BBwP4Npmp+tOG99JnFcAFTVkfZ8DPg8gx/Mk/5ZnAPmqurxtv8gg8BftXGtd8B/FdjeftP/BgYrZfesc02rYXg17/GrfN/Xfht+JfDSwl/FzjRJwmDR2sGq+tjQoYkeW5KpJBvb9huBdzP4xdZEr8yujlecJ3lTkrcsbAO/CjzNhH8Wq+p/AIeT/K3WdC3wDKs5rjPgFw03AH/BYB70X653Pcuo/9PAUeD/MvgJeyuDucy9wHPt+fzWNwzuGvo28E1ger3rX2Rcf5/BX/+eAva3xw2TPjbg7wBfb+N6GvhXrf1twBPALPBfgHNa+7ltf7Ydf9t6j2GMMV4NPNzLuNoYvtEeBxZyYtI/i63WHcC+9nn8r8B5qzkuV7JKUqfWe4pGkrRGDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjr1/wArM4UXg/BNywAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d6920d1780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\").env\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import keras.layers as L\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем небольшую сетку - два скрытых слоя со 100 нейронами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "network = Sequential()\n",
    "network.add(L.InputLayer(state_dim))\n",
    "\n",
    "network.add(L.Dense(200, activation='relu'))\n",
    "#network.add(L.Dense(100, activation='relu'))\n",
    "#network.add(L.Dense(50, activation='softmax'))\n",
    "network.add(L.Dense(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_action(state, epsilon=0):\n",
    "    \"\"\"\n",
    "    sample actions with epsilon-greedy policy\n",
    "    recap: with p = epsilon pick random action, else pick action with highest Q(s,a)\n",
    "    \"\"\"\n",
    "    \n",
    "    q_values = network.predict(state[None])[0]\n",
    "    \n",
    "    action = None\n",
    "    if epsilon > random.random():\n",
    "        action = random.choice(range(n_actions))\n",
    "    else:\n",
    "        action = np.argmax(q_values)\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=0.0 tests passed\n",
      "e=0.1 tests passed\n",
      "e=0.5 tests passed\n",
      "e=1.0 tests passed\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import random\n",
    "\n",
    "assert network.output_shape == (None, n_actions), \"please make sure your model maps state s -> [Q(s,a0), ..., Q(s, a_last)]\"\n",
    "assert network.layers[-1].activation == keras.activations.linear, \"please make sure you predict q-values without nonlinearity\"\n",
    "\n",
    "# test epsilon-greedy exploration\n",
    "s = env.reset()\n",
    "assert np.shape(get_action(s)) == (), \"please return just one action (integer)\"\n",
    "for eps in [0., 0.1, 0.5, 1.0]:\n",
    "    state_frequencies = np.bincount([get_action(s, epsilon=eps) for i in range(10000)], minlength=n_actions)\n",
    "    best_action = state_frequencies.argmax()\n",
    "    assert abs(state_frequencies[best_action] - 10000 * (1 - eps + eps / n_actions)) < 200\n",
    "    for other_action in range(n_actions):\n",
    "        if other_action != best_action:\n",
    "            assert abs(state_frequencies[other_action] - 10000 * (eps / n_actions)) < 200\n",
    "    print('e=%.1f tests passed'%eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Будем обучать нашу сеть, используя так называемую функцию потерь TD (time-difference):\n",
    "\n",
    "$$ L = { 1 \\over N} \\sum_i (Q_{\\theta}(s,a) - [r(s,a) + \\gamma \\cdot max_{a'} Q_{-}(s', a')]) ^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хитрость здесь заключается в том, что мы используем \"отложенное\" состояние сети $Q_{-}(s',a')$, которое не изменяется в процессе распространения ошибки по текущему состоянию сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create placeholders for the <s, a, r, s'> tuple and a special indicator for game end (is_done = True)\n",
    "states_ph = tf.placeholder('float32', shape=(None,) + state_dim)\n",
    "actions_ph = tf.placeholder('int32', shape=[None])\n",
    "rewards_ph = tf.placeholder('float32', shape=[None])\n",
    "next_states_ph = tf.placeholder('float32', shape=(None,) + state_dim)\n",
    "is_done_ph = tf.placeholder('bool', shape=[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get q-values for all actions in current states\n",
    "predicted_qvalues = network(states_ph)\n",
    "\n",
    "#select q-values for chosen actions\n",
    "predicted_qvalues_for_actions = tf.reduce_sum(predicted_qvalues * tf.one_hot(actions_ph, n_actions), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "# compute q-values for all actions in next states\n",
    "#predicted_next_qvalues = ###YOUR CODE - apply network to get q-values for next_states_ph\n",
    "predicted_next_qvalues = network(next_states_ph)\n",
    "\n",
    "# compute V*(next_states) using predicted next q-values\n",
    "next_state_values = tf.reduce_max(predicted_next_qvalues, axis=1)\n",
    "\n",
    "# compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
    "target_qvalues_for_actions = rewards_ph + gamma * next_state_values\n",
    "\n",
    "# at the last state we shall use simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "target_qvalues_for_actions = tf.where(is_done_ph, rewards_ph, target_qvalues_for_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean squared error loss to minimize\n",
    "loss = (predicted_qvalues_for_actions - tf.stop_gradient(target_qvalues_for_actions)) ** 2\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "# training function that resembles agent.update(state, action, reward, next_state) from tabular agent\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tf.gradients(loss, [predicted_qvalues_for_actions])[0] is not None, \"make sure you update q-values for chosen actions and not just all actions\"\n",
    "assert tf.gradients(loss, [predicted_next_qvalues])[0] is None, \"make sure you don't propagate gradient w.r.t. Q_(s',a')\"\n",
    "assert predicted_next_qvalues.shape.ndims == 2, \"make sure you predicted q-values for all actions in next state\"\n",
    "assert next_state_values.shape.ndims == 1, \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n",
    "assert target_qvalues_for_actions.shape.ndims == 1, \"there's something wrong with target q-values, they must be a vector\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь проверим как работает наш нейроагент"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000, epsilon=0, train=False):\n",
    "    \"\"\"play env with approximate q-learning agent and train it at the same time\"\"\"\n",
    "    total_reward = 0\n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        a = get_action(s, epsilon=epsilon)       \n",
    "        next_s, r, done, _ = env.step(a)\n",
    "        \n",
    "        if train:\n",
    "            sess.run(train_step,{\n",
    "                states_ph: [s], actions_ph: [a], rewards_ph: [r], \n",
    "                next_states_ph: [next_s], is_done_ph: [done]\n",
    "            })\n",
    "\n",
    "        total_reward += r\n",
    "        s = next_s\n",
    "        if done: break\n",
    "            \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #0\tmean reward = 24.450\tepsilon = 0.500\n",
      "epoch #1\tmean reward = 13.920\tepsilon = 0.495\n",
      "epoch #2\tmean reward = 13.050\tepsilon = 0.490\n",
      "epoch #3\tmean reward = 13.360\tepsilon = 0.485\n",
      "epoch #4\tmean reward = 14.600\tepsilon = 0.480\n",
      "epoch #5\tmean reward = 25.430\tepsilon = 0.475\n",
      "epoch #6\tmean reward = 16.640\tepsilon = 0.471\n",
      "epoch #7\tmean reward = 17.500\tepsilon = 0.466\n",
      "epoch #8\tmean reward = 12.890\tepsilon = 0.461\n",
      "epoch #9\tmean reward = 14.240\tepsilon = 0.457\n",
      "epoch #10\tmean reward = 17.080\tepsilon = 0.452\n",
      "epoch #11\tmean reward = 11.970\tepsilon = 0.448\n",
      "epoch #12\tmean reward = 14.380\tepsilon = 0.443\n",
      "epoch #13\tmean reward = 16.840\tepsilon = 0.439\n",
      "epoch #14\tmean reward = 13.870\tepsilon = 0.434\n",
      "epoch #15\tmean reward = 13.140\tepsilon = 0.430\n",
      "epoch #16\tmean reward = 17.350\tepsilon = 0.426\n",
      "epoch #17\tmean reward = 13.010\tepsilon = 0.421\n",
      "epoch #18\tmean reward = 18.300\tepsilon = 0.417\n",
      "epoch #19\tmean reward = 39.830\tepsilon = 0.413\n",
      "epoch #20\tmean reward = 28.070\tepsilon = 0.409\n",
      "epoch #21\tmean reward = 30.770\tepsilon = 0.405\n",
      "epoch #22\tmean reward = 33.530\tepsilon = 0.401\n",
      "epoch #23\tmean reward = 47.600\tepsilon = 0.397\n",
      "epoch #24\tmean reward = 41.810\tepsilon = 0.393\n",
      "epoch #25\tmean reward = 43.760\tepsilon = 0.389\n",
      "epoch #26\tmean reward = 47.780\tepsilon = 0.385\n",
      "epoch #27\tmean reward = 43.870\tepsilon = 0.381\n",
      "epoch #28\tmean reward = 45.050\tepsilon = 0.377\n",
      "epoch #29\tmean reward = 48.500\tepsilon = 0.374\n",
      "epoch #30\tmean reward = 63.570\tepsilon = 0.370\n",
      "epoch #31\tmean reward = 67.810\tepsilon = 0.366\n",
      "epoch #32\tmean reward = 83.290\tepsilon = 0.362\n",
      "epoch #33\tmean reward = 82.150\tepsilon = 0.359\n",
      "epoch #34\tmean reward = 93.870\tepsilon = 0.355\n",
      "epoch #35\tmean reward = 90.960\tepsilon = 0.352\n",
      "epoch #36\tmean reward = 106.980\tepsilon = 0.348\n",
      "epoch #37\tmean reward = 85.690\tepsilon = 0.345\n",
      "epoch #38\tmean reward = 110.550\tepsilon = 0.341\n",
      "epoch #39\tmean reward = 98.970\tepsilon = 0.338\n",
      "epoch #40\tmean reward = 118.890\tepsilon = 0.334\n",
      "epoch #41\tmean reward = 150.440\tepsilon = 0.331\n",
      "epoch #42\tmean reward = 135.510\tepsilon = 0.328\n",
      "epoch #43\tmean reward = 105.670\tepsilon = 0.325\n",
      "epoch #44\tmean reward = 63.020\tepsilon = 0.321\n",
      "epoch #45\tmean reward = 135.040\tepsilon = 0.318\n",
      "epoch #46\tmean reward = 174.880\tepsilon = 0.315\n",
      "epoch #47\tmean reward = 142.080\tepsilon = 0.312\n",
      "epoch #48\tmean reward = 35.880\tepsilon = 0.309\n",
      "epoch #49\tmean reward = 83.200\tepsilon = 0.306\n",
      "epoch #50\tmean reward = 184.970\tepsilon = 0.303\n",
      "epoch #51\tmean reward = 161.860\tepsilon = 0.299\n",
      "epoch #52\tmean reward = 127.240\tepsilon = 0.296\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-a26318acddbe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0msession_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mgenerate_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession_rewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-a26318acddbe>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0msession_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mgenerate_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession_rewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-fe34f7834adf>\u001b[0m in \u001b[0;36mgenerate_session\u001b[1;34m(t_max, epsilon, train)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mnext_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-03cb833fcc2f>\u001b[0m in \u001b[0;36mget_action\u001b[1;34m(state, epsilon)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \"\"\"\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[0;32m    889\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 891\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[0;32m   1570\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1571\u001b[0m         return self._predict_loop(f, ins,\n\u001b[1;32m-> 1572\u001b[1;33m                                   batch_size=batch_size, verbose=verbose)\n\u001b[0m\u001b[0;32m   1573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1574\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[1;34m(self, f, ins, batch_size, verbose)\u001b[0m\n\u001b[0;32m   1200\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1202\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1203\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1204\u001b[0m                 \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2073\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2074\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m-> 2075\u001b[1;33m                               feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m   2076\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2077\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1091\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1092\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1093\u001b[1;33m             \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1094\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1095\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \"\"\"\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epsilon = 0.5\n",
    "for i in range(1000):\n",
    "    session_rewards = [generate_session(epsilon=epsilon, train=True) for _ in range(100)]\n",
    "    print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(i, np.mean(session_rewards), epsilon))\n",
    "    \n",
    "    epsilon *= 0.99\n",
    "    assert epsilon >= 1e-4, \"Make sure epsilon is always nonzero during training\"\n",
    "    \n",
    "    if np.mean(session_rewards) > 300:\n",
    "        print (\"You Win!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
